{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "806c8944-33f5-4219-ab8c-c5582a59a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "from tensorflow.keras.utils import load_img,img_to_array\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import RandomCrop, RandomFlip, RandomRotation, RandomContrast\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a40f8-1356-4197-9e74-3e335c6061a9",
   "metadata": {},
   "source": [
    "# Train MobileNetV3\n",
    "To train our neural network we need:\n",
    " - Load the data\n",
    " - Downscale data and prepare it for input\n",
    " - create some neccessary functions to show image, show scores and etc\n",
    " - Load MobileNetV3 pretrained model without top\n",
    " - Prepare Data augmentation layers\n",
    " - Prepare output layers\n",
    " - Set neccessary parameters\n",
    " - Train the model\n",
    " - evaluate the model performance\n",
    " - create pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4c41f-f8d7-4386-bf8e-7e4642c1a0ac",
   "metadata": {},
   "source": [
    "#### parameters initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "590f9b31-668a-4661-b852-f0142b3e3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to set seed for any randomizing that will happen during process\n",
    "SEED = 44\n",
    "# choose how much times we want data to be shown\n",
    "epochs = 40\n",
    "BATCH_SIZE = 32\n",
    "# validation frequency so validation wont be run every epoch\n",
    "VAL_FREQ = 6\n",
    "OPTIMIZER = 'adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea1ffd7-db15-4aed-a4bb-2c5f654168cb",
   "metadata": {},
   "source": [
    "# Create Data Loader\n",
    "We need data loader to make loading batches computationally efficient and avoid memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0be9e5c9-83e6-4fd0-bb4b-eb3997331bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_directory(filename):\n",
    "    data = tf.keras.utils.image_dataset_from_directory(filename,batch_size=32,image_size=(224,224),seed=seed)\n",
    "    return data\n",
    "\n",
    "# We split our dataset into train and validation sets\n",
    "def split_dir_dataset(data):\n",
    "    # scale down rgb from 255 to 0\n",
    "    data = data.map(lambda x,y: (x/255,y))\n",
    "    \n",
    "    # percent size of train and val sets\n",
    "    train_size = int(len(data)*.85)\n",
    "    val_size = int(len(data)*.15)\n",
    "    \n",
    "    # take that size images from dataset\n",
    "    train = data.take(train_size)\n",
    "    val = data.skip(train_size).take(val_size)\n",
    "    \n",
    "    print(f'training shape: {train.as_numpy_iterator().next()[0].shape}')\n",
    "    return train,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0289d2a2-a6f2-47c5-b9e2-6acee9b18b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 49842 files belonging to 9 classes.\n",
      "training shape: (32, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# path to our dataset\n",
    "dataset_path = './multiclass'\n",
    "\n",
    "data = load_from_directory(dataset_path)\n",
    "\n",
    "train, val = split_dir_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba9f5f-ae57-43e6-b4c0-570e9ca500f4",
   "metadata": {},
   "source": [
    "# Necessary Functions\n",
    "Now we have to initialize necessary functions that we will use later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8def194e-d24a-4895-9e79-98fa3a37bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples_dataset(data):\n",
    "    class_names = data.class_names\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for images, labels in data.take(1):\n",
    "      for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "          \n",
    "def show_results(hist,mode='acc'):\n",
    "    if mode == 'acc' or mode == 'accuracy':\n",
    "        mode = 'accuracy'\n",
    "        val_mode = 'val_accuracy'\n",
    "    elif mode == 'loss':\n",
    "        mode = \"loss\"\n",
    "        val_mode = \"val_loss\"\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(hist.history[mode], color='teal', label=mode)\n",
    "    plt.plot(hist.history[val_mode], color='orange', label=val_mode)\n",
    "    fig.suptitle(mode, fontsize=20)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "    \n",
    "def prediction_dir_dataset(model,test):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch in test.as_numpy_iterator(): \n",
    "        X, y = batch\n",
    "        yhat = model.predict(X)\n",
    "        yhat = np.argmax(yhat,axis=1)\n",
    "        y_true.append(y)\n",
    "        y_pred.append(yhat)\n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    return y_true,y_pred\n",
    "\n",
    "def scores(y_true,y_pred,show_confusion_matrix=True):\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    precision_weighted = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    recall_weighted = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    if show_confusion_matrix == True:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "    print(f\"precision weighted: {'{:.3f}'.format(precision_weighted)}\\nprecision micro: {precision_micro} \")\n",
    "    print(f'recall weighted: {recall_weighted}\\nrecall micro: {recall_micro} ')\n",
    "    print(f'f1 weighted: {\"{:.3f}\".format(f1_weighted)}\\nf1 micro: {f1_micro} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef2e07-b28c-4300-abdb-5d5125eedf12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0836c8e-ec32-450a-a14e-81043795ec54",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6d1e494-3b3b-413c-9d2e-03fd51d7e596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNetV3Large(weights='imagenet',include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03539b23-0275-4a21-a05f-f033738f8cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global spatial avg pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Fully connected layer\n",
    "x = Dense(1024,activation='relu')(x)\n",
    "\n",
    "# We have 2 classes either it is suitable for children or not but for now we train for multiclass classification\n",
    "# which means softmax distribution over 9 classes\n",
    "predictions = Dense(9,activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a718e5-4092-4b69-b2c1-56c4e4628453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional MobileNetV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "# add Early stopping callback to stop model before it overfits the data\n",
    "callback = EarlyStopping(monitor='val_loss',patience=5,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bbdc28-d7c6-4c6e-8666-dce53d7b460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dd3f2d-0963-44ea-b93c-5b268170a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize this so computing is more efficient\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train = train.cache()\n",
    "train = train.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4cdf0-257a-425a-bda8-0e6e963862bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "epochs = 10\n",
    "hist = model.fit(train,\n",
    "                 epochs = epochs,\n",
    "                 validation_data=val,\n",
    "                 callbacks = [callback],\n",
    "                 validation_freq=VAL_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e174f2e-f6ce-4a5e-a2e5-4bae94fcfccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# at this point, the top layers are well trained and we can start fine-tuning\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# convolutional layers from MobileNet V3. We will freeze the bottom N layers\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# and train the remaining top layers.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# let's visualize layer names and layer indices to see how many layers\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# we should freeze:\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mbase_model\u001b[49m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m      8\u001b[0m    \u001b[38;5;28mprint\u001b[39m(i, layer\u001b[38;5;241m.\u001b[39mname)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from MobileNet V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd52953-7119-44ff-b33b-efdc56e19888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7fe1f-8b52-46ba-a82d-a433337cd862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=OPTIMIZER), loss=tf.losses.SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1277ec-64a1-40d8-a0ad-e86fba9ffd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train our model again, this time fine-tuning the convolutional layers\n",
    "# alongside the top Dense layers\n",
    "epochs = 50\n",
    "hist_unfreezed = model.fit(train,\n",
    "                           epochs = epochs,\n",
    "                           validation_data=val,\n",
    "                           callbacks = [callback],\n",
    "                           validation_freq=VAL_FREQ)\n",
    "\n",
    "#  https://keras.io/api/applications/#usage-examples-for-image-classification-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30db2c-1935-4d50-9577-e025ed26c642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baf204ca-559d-41fe-91ca-4af97d693483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(img_path):\n",
    "    # img_path = 'dawg.jpg'\n",
    "    img = load_img(img_path, target_size=(224, 224))\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    \n",
    "    preds = model.predict(x)\n",
    "    # decode the results into a list of tuples (class, description, probability)\n",
    "    # (one such list for each sample in the batch)\n",
    "    print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc412c0a-7197-4032-8970-bd29101767ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
