{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806c8944-33f5-4219-ab8c-c5582a59a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score, fbeta_score, precision_score, recall_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "from tensorflow.keras.utils import load_img,img_to_array\n",
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Input,Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import RandomCrop, RandomFlip, RandomRotation, RandomContrast\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a40f8-1356-4197-9e74-3e335c6061a9",
   "metadata": {},
   "source": [
    "# Train MobileNetV3\n",
    "To train our neural network we need:\n",
    " - Load the data\n",
    " - Downscale data and prepare it for input\n",
    " - create some neccessary functions to show image, show scores and etc\n",
    " - Load MobileNetV3 pretrained model without top\n",
    " - Prepare Data augmentation layers\n",
    " - Prepare output layers\n",
    " - Set neccessary parameters\n",
    " - Train the model\n",
    " - evaluate the model performance\n",
    " - create pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd4c41f-f8d7-4386-bf8e-7e4642c1a0ac",
   "metadata": {},
   "source": [
    "#### parameters initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590f9b31-668a-4661-b852-f0142b3e3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to set seed for any randomizing that will happen during process\n",
    "SEED = 44\n",
    "# choose how much times we want data to be shown\n",
    "epochs = 5\n",
    "BATCH_SIZE = 32\n",
    "# validation frequency so validation wont be run every epoch\n",
    "VAL_FREQ = 2\n",
    "OPTIMIZER = 'adam'\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "shape =  112\n",
    "shape_mobilenet = 96\n",
    "input_shape = (shape,shape,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea1ffd7-db15-4aed-a4bb-2c5f654168cb",
   "metadata": {},
   "source": [
    "# Create Data Loader\n",
    "We need data loader to make loading batches computationally efficient and avoid memory issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be9e5c9-83e6-4fd0-bb4b-eb3997331bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_directory(filename):\n",
    "    data = tf.keras.utils.image_dataset_from_directory(filename,\n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       image_size=(shape,shape),\n",
    "                                                       seed=SEED,labels=\"inferred\",\n",
    "                                                       label_mode=\"int\",\n",
    "                                                       shuffle=True)\n",
    "    return data\n",
    "\n",
    "# We split our dataset into train and validation sets\n",
    "def split_dir_dataset(data):\n",
    "    # scale down rgb from 255 to 0\n",
    "    data = data.map(lambda x,y: (x/255,y))\n",
    "    \n",
    "    # percent size of train and val sets\n",
    "    train_size = int(len(data)* .85)\n",
    "    test_size = int(len(data) * .10)\n",
    "    val_size = int(len(data)*.05)\n",
    "    \n",
    "    \n",
    "    # take that size images from dataset\n",
    "    train = data.take(train_size)\n",
    "    val = data.skip(train_size).take(val_size)\n",
    "    test = data.skip(train_size+val_size).take(test_size)\n",
    "    \n",
    "    \n",
    "    print(f'training shape: {train.as_numpy_iterator().next()[0].shape}')\n",
    "    return train,test,val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0289d2a2-a6f2-47c5-b9e2-6acee9b18b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1218220 files belonging to 2 classes.\n",
      "training shape: (32, 112, 112, 3)\n"
     ]
    }
   ],
   "source": [
    "# path to our dataset\n",
    "dataset_path = '../multiclass'\n",
    "\n",
    "data = load_from_directory(dataset_path)\n",
    "\n",
    "train,test, val = split_dir_dataset(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b3bd95c-e05b-41b3-b89a-df6889b05b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "output_len = len(data.class_names)\n",
    "print(output_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba9f5f-ae57-43e6-b4c0-570e9ca500f4",
   "metadata": {},
   "source": [
    "# Necessary Functions\n",
    "Now we have to initialize necessary functions that we will use later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8def194e-d24a-4895-9e79-98fa3a37bc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples_dataset(data):\n",
    "    class_names = data.class_names\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for images, labels in data.take(1):\n",
    "      for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n",
    "          \n",
    "def show_results(hist,mode='acc'):\n",
    "    if mode == 'acc' or mode == 'accuracy':\n",
    "        mode = 'accuracy'\n",
    "        val_mode = 'val_accuracy'\n",
    "    elif mode == 'loss':\n",
    "        mode = \"loss\"\n",
    "        val_mode = \"val_loss\"\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(hist.history[mode], color='teal', label=mode)\n",
    "    plt.plot(hist.history[val_mode], color='orange', label=val_mode)\n",
    "    fig.suptitle(mode, fontsize=20)\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "    \n",
    "def prediction_dir_dataset(model,test,data):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    length = int(len(data) * .10)\n",
    "    index = 0\n",
    "    for batch in test: \n",
    "        X, y = batch\n",
    "        yhat = model.predict(X)\n",
    "        yhat = np.argmax(yhat,axis=1)\n",
    "        y_true.append(y)\n",
    "        y_pred.append(yhat)\n",
    "        print(f\"Progress: {index}/{length}\", end='\\r')\n",
    "        index += len(X)\n",
    "    y_true = np.array(y_true)\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "def scores(y_true,y_pred,show_confusion_matrix=True):\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    precision_weighted = precision_score(y_true, y_pred, average='weighted')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    recall_weighted = recall_score(y_true, y_pred, average='weighted')\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "    f2_micro = fbeta_score(y_true, y_pred, beta=2, average='micro')\n",
    "    f2_weighted = fbeta_score(y_true, y_pred, beta=2, average='weighted')\n",
    "    \n",
    "    if show_confusion_matrix == True:\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot()\n",
    "        plt.show()\n",
    "    print(f\"precision weighted: {'{:.3f}'.format(precision_weighted)}\\nprecision micro: {precision_micro} \")\n",
    "    print(f'recall weighted: {recall_weighted}\\nrecall micro: {recall_micro} ')\n",
    "    print(f'f1 weighted: {\"{:.3f}\".format(f1_weighted)}\\nf1 micro: {f1_micro} ')\n",
    "    print(f'f2 weighted: {\"{:.3f}\".format(f2_weighted)}\\nf2 micro: {f2_micro} ')\n",
    "    \n",
    "\n",
    "# To-Do pretty confusion matrix with percentage\n",
    "\n",
    "    # normalize='all'\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# # Normalise\n",
    "# cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "# fig, ax = plt.subplots(figsize=(10,10))\n",
    "# sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=target_names, yticklabels=target_names)\n",
    "# plt.ylabel('Actual')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24bc644-fad6-4881-8fa3-5cac25e63262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45fe778d-0385-473c-bd28-4c51178d1910",
   "metadata": {},
   "source": [
    "# Add Data augmentation to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06bbae64-30d5-4b13-a503-9779033b8788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(X):\n",
    "    X = RandomCrop(height=shape_mobilenet,width=shape_mobilenet,seed=SEED)(X)\n",
    "    X = RandomFlip(mode=\"horizontal_and_vertical\",seed=SEED)(X)\n",
    "    X = RandomRotation(factor=0.9)(X)\n",
    "    X = RandomContrast(0.8,seed=SEED)(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990625c-492b-4ee0-807f-003ed471467f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0836c8e-ec32-450a-a14e-81043795ec54",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d1e494-3b3b-413c-9d2e-03fd51d7e596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "base_model = MobileNetV3Large(input_shape=(shape_mobilenet,shape_mobilenet,3),weights='imagenet',include_top=False,include_preprocessing=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85eb8595-9143-48f8-a3c1-b8a7b0bdef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = 768\n",
    "model_name = f'mobilenet_best_l2_112_{num_neurons}_lr00001.h5'\n",
    "\n",
    "# initialize input shape layer for data augmentation block\n",
    "x_input = Input(input_shape)\n",
    "x = data_augment(x_input)\n",
    "\n",
    "# connect data augmentation block to base model\n",
    "x = base_model(x)\n",
    "\n",
    "\n",
    "# Global spatial avg pooling layer\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Fully connected layer\n",
    "x = Dense(num_neurons,activation='relu',kernel_regularizer=l2(0.03))(x)\n",
    "x = Dropout(0.6)(x)\n",
    "\n",
    "\n",
    "\n",
    "# We have 2 classes either it is suitable for children or not\n",
    "predictions = Dense(output_len,activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=x_input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97a718e5-4092-4b69-b2c1-56c4e4628453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional MobileNetV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87bbdc28-d7c6-4c6e-8666-dce53d7b460f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 112, 112, 3)]     0         \n",
      "                                                                 \n",
      " random_crop (RandomCrop)    (None, 96, 96, 3)         0         \n",
      "                                                                 \n",
      " random_flip (RandomFlip)    (None, 96, 96, 3)         0         \n",
      "                                                                 \n",
      " random_rotation (RandomRota  (None, 96, 96, 3)        0         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " random_contrast (RandomCont  (None, 96, 96, 3)        0         \n",
      " rast)                                                           \n",
      "                                                                 \n",
      " MobilenetV3large (Functiona  (None, 3, 3, 960)        2996352   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 960)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 768)               738048    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 768)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,735,938\n",
      "Trainable params: 739,586\n",
      "Non-trainable params: 2,996,352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# visualize model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08dd3f2d-0963-44ea-b93c-5b268170a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize this so computing is more efficient\n",
    "# AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# # train = train.cache()\n",
    "# train = train.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint(model_name, monitor='val_loss', save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bd4cdf0-257a-425a-bda8-0e6e963862bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32359/32359 [==============================] - 900s 28ms/step - loss: 0.4976 - accuracy: 0.8408 - val_loss: 0.3746 - val_accuracy: 0.8832\n"
     ]
    }
   ],
   "source": [
    "# train the model on the new data for a few epochs\n",
    "epochs = 1\n",
    "\n",
    "hist = model.fit(train,\n",
    "                 epochs = epochs,\n",
    "                 validation_data=val,\n",
    "                 callbacks = [checkpoint,early_stopping]\n",
    "                 # ,validation_freq=VAL_FREQ\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbf79d8-5778-4fd1-89df-3fddd9edd28d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cf551b6-98aa-4476-98b3-aa45156984fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 112, 112, 3)]     0         \n",
      "                                                                 \n",
      " random_crop (RandomCrop)    (None, 96, 96, 3)         0         \n",
      "                                                                 \n",
      " random_flip (RandomFlip)    (None, 96, 96, 3)         0         \n",
      "                                                                 \n",
      " random_rotation (RandomRota  (None, 96, 96, 3)        0         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " random_contrast (RandomCont  (None, 96, 96, 3)        0         \n",
      " rast)                                                           \n",
      "                                                                 \n",
      " MobilenetV3large (Functiona  (None, 3, 3, 960)        2996352   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 960)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 768)               738048    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 768)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,735,938\n",
      "Trainable params: 739,586\n",
      "Non-trainable params: 2,996,352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e174f2e-f6ce-4a5e-a2e5-4bae94fcfccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_2\n",
      "1 random_crop\n",
      "2 random_flip\n",
      "3 random_rotation\n",
      "4 random_contrast\n",
      "5 MobilenetV3large\n",
      "6 global_average_pooling2d\n",
      "7 dense\n",
      "8 dropout\n",
      "9 dense_1\n"
     ]
    }
   ],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from MobileNet V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b6894ce-82cf-422b-8b8d-8e1acfc2092a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 112, 112, 3)]     0         \n",
      "                                                                 \n",
      " random_crop (RandomCrop)    (None, 96, 96, 3)         0         \n",
      "                                                                 \n",
      " random_flip (RandomFlip)    (None, 96, 96, 3)         0         \n",
      "                                                                 \n",
      " random_rotation (RandomRota  (None, 96, 96, 3)        0         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " random_contrast (RandomCont  (None, 96, 96, 3)        0         \n",
      " rast)                                                           \n",
      "                                                                 \n",
      " MobilenetV3large (Functiona  (None, 3, 3, 960)        2996352   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 960)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 768)               738048    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 768)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,735,938\n",
      "Trainable params: 739,586\n",
      "Non-trainable params: 2,996,352\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afd52953-7119-44ff-b33b-efdc56e19888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b864c68-5b8e-4256-b103-41f2e04040ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(model_name, monitor='val_loss', save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fec7fe1f-8b52-46ba-a82d-a433337cd862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use adam with a low learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78d8c2d7-9380-4b19-9909-b5ad8f9659cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 112, 112, 3)]     0         \n",
      "                                                                 \n",
      " random_crop (RandomCrop)    (None, 96, 96, 3)         0         \n",
      "                                                                 \n",
      " random_flip (RandomFlip)    (None, 96, 96, 3)         0         \n",
      "                                                                 \n",
      " random_rotation (RandomRota  (None, 96, 96, 3)        0         \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " random_contrast (RandomCont  (None, 96, 96, 3)        0         \n",
      " rast)                                                           \n",
      "                                                                 \n",
      " MobilenetV3large (Functiona  (None, 3, 3, 960)        2996352   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 960)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 768)               738048    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 768)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,735,938\n",
      "Trainable params: 3,711,538\n",
      "Non-trainable params: 24,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1277ec-64a1-40d8-a0ad-e86fba9ffd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "32359/32359 [==============================] - 1915s 59ms/step - loss: 0.1603 - accuracy: 0.9451 - val_loss: 0.2131 - val_accuracy: 0.9208\n",
      "Epoch 2/5\n",
      "32359/32359 [==============================] - 1912s 59ms/step - loss: 0.1213 - accuracy: 0.9585 - val_loss: 0.4071 - val_accuracy: 0.9377\n",
      "Epoch 3/5\n",
      " 4299/32359 [==>...........................] - ETA: 24:23 - loss: 0.1124 - accuracy: 0.9611"
     ]
    }
   ],
   "source": [
    "# we train our model again, this time fine-tuning the convolutional layers\n",
    "# alongside the top Dense layers\n",
    "epochs = 5\n",
    "hist_unfreezed = model.fit(train,\n",
    "                 epochs = epochs,\n",
    "                 validation_data=val,\n",
    "                 callbacks = [checkpoint,early_stopping]\n",
    "                 # ,validation_freq=VAL_FREQ\n",
    "                )\n",
    "#  https://keras.io/api/applications/#usage-examples-for-image-classification-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30db2c-1935-4d50-9577-e025ed26c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    return model\n",
    "\n",
    "# model = load_model('mobilenet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6353be-79a7-46c6-87c7-3c86f63bb6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf204ca-559d-41fe-91ca-4af97d693483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(img_path):\n",
    "    # img_path = 'dawg.jpg'\n",
    "    img = load_img(img_path, target_size=(128, 128))\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    preds = model.predict(x)\n",
    "    \n",
    "    # decode the results into a list of tuples (class, description, probability)\n",
    "    # (one such list for each sample in the batch)\n",
    "    predicted_idx = np.argmax(preds)\n",
    "    predicted_class = data.class_names[predicted_idx]\n",
    "    print(predicted_class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc412c0a-7197-4032-8970-bd29101767ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(\"images (1).jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7469a9-642b-45b9-9b0f-4064d9ceacea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5a62a-ef9a-4c99-8545-58bbb514ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063061d8-8d2f-47e0-8ca2-023a1e92513a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true,y_pred = prediction_dir_dataset(model,test,data)\n",
    "scores(y_true,y_pred,show_confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d6494c-55af-447d-bcbe-796f76a260cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores(y_true,y_pred,show_confusion_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b5c5c-7e7a-461a-ac58-569d750acc66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b9e6c8-0993-4768-8750-fdb8e3a2dae6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875abd9f-9ac0-49d1-9767-02db9d90d327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "558dd086-f4be-42e5-92c7-37a834554a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def testing():\n",
    "    i = 2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681031f-0693-469b-9bd3-3b78d93e1c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started loop\n",
      "Tensor(\"args_2:0\", shape=(None, 112, 112, 3), dtype=float32) Tensor(\"args_3:0\", shape=(None,), dtype=int32)\n",
      "WARNING:tensorflow:From C:\\Users\\Saba\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:798: take_while (from tensorflow.python.data.experimental.ops.take_while_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.take_while(...)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023227e-69c4-4c14-b989-d26d984e6816",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
