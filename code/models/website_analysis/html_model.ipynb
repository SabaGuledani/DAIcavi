{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e325ae-d79a-4ba3-bfac-ae37a8ede766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Saba\\.conda\\envs\\AI\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import sqlite3\n",
    "import jsonlines\n",
    "from collections import Counter\n",
    "from unicodedata import normalize\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Embedding, Conv2D, MaxPooling2D, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Clear memory\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e669f84-aa46-46e8-b05c-c93cc9209d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_vocab_size = 15000\n",
    "html_max_length = 128\n",
    "embed_dimension = 2\n",
    "batch_size = 8\n",
    "num_filters = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7738e6-a2cb-416f-a7c6-5a8f8a2a7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_and_average(seq_list):\n",
    "    lengths = [len(doc.ids) for doc in seq_list]\n",
    "    lengths.sort()\n",
    "    lengths_avg = sum(lengths)/len(lengths)\n",
    "    median = lengths[len(lengths)//2]\n",
    "    print(f\"median: {median}\")\n",
    "    print(f\"average length: {int(lengths_avg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4017d5b4-feee-4618-a4f5-9a16944b2af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list loaded successfully\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "# Load the list from the pickle file\n",
    "with open('training_data.pkl', 'rb') as f:\n",
    "    training_data = pickle.load(f)\n",
    "    # random.shuffle(training_data)\n",
    "    print(\"list loaded successfully\")\n",
    "    print(len(training_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65669613-7d78-4c86-985f-52400cd37b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and test sets\n",
    "train_split = 0.8\n",
    "test_split = 1-train_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aecdc825-d1c9-41fa-9220-ccd91e0b7bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 400\n"
     ]
    }
   ],
   "source": [
    "train = training_data[: int(len(training_data) * train_split)]\n",
    "test = training_data[int(len(training_data) * train_split):]\n",
    "print(len(train),len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98bc21c-13e0-4b5e-8164-1aede2948b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load X and Y s of dataset of tuples which are (html,label) \n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "for elem in train:\n",
    "    x_train.append(elem[0])\n",
    "    if elem[1] == \"Adult\":\n",
    "        y_train.append(1)\n",
    "    elif elem[1] == \"Benign\":\n",
    "        y_train.append(0)\n",
    "\n",
    "for elem in test:\n",
    "    x_test.append(elem[0])\n",
    "    if elem[1] == \"Adult\":\n",
    "        y_test.append(1)\n",
    "    elif elem[1] == \"Benign\":\n",
    "        y_test.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9303a053-3c6a-4d6d-b366-8294ba867582",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train_from_iterator(x_train+x_test,\n",
    "                              vocab_size=tokenizer_vocab_size, \n",
    "                              min_frequency=2,\n",
    "                              special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87017445-76b5-4eb1-8c5c-b218b7933dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = \n",
    "# encoded_html_docs = [tokenizer.encode(doc) for doc in loaded_html_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cd681c1-1887-4a0a-a62b-24b4940b7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(path):\n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    tokenizer.save_model(path)\n",
    "    print(\"tokenizer saved successfully\")\n",
    "\n",
    "# Load model\n",
    "def load_tokenizer(path):\n",
    "    tokenizer = ByteLevelBPETokenizer(f\"{path}/vocab.json\", f\"{path}/merges.txt\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef3b4e68-cff2-4ca5-9804-a1fa88d95c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer saved successfully\n"
     ]
    }
   ],
   "source": [
    "save_tokenizer(\"bpe_tokenizer_20k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22ac2c34-86bf-4ac0-a69a-3044734f95ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, htmls, labels, batch_size, tokenizer):\n",
    "        self.htmls = htmls\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.indexes = np.arange(len(htmls))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.htmls) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_htmls = [self.htmls[i] for i in batch_indexes]\n",
    "        batch_labels = [self.labels[i] for i in batch_indexes]\n",
    "        \n",
    "        \n",
    "        html_input_data = self._preprocess_html_doc(batch_htmls)\n",
    "        \n",
    "        return html_input_data, np.array(batch_labels)\n",
    "\n",
    "    \n",
    "    def encode_and_pad_sequence(self,sequence, max_length, pad_token=\"<pad>\"):\n",
    "        pad_token_id = 3\n",
    "    \n",
    "       \n",
    "        token_ids =  tokenizer.encode(sequence).ids\n",
    "        if len(token_ids) < max_length:\n",
    "            # Pad with <pad> token\n",
    "            token_ids.extend([pad_token_id] * (max_length - len(token_ids)))\n",
    "        return token_ids[:max_length]\n",
    "    \n",
    "    def _preprocess_html_doc(self, htmls):\n",
    "        html_sequences = [self.encode_and_pad_sequence(sequence,html_max_length) for sequence in htmls]\n",
    "        \n",
    "        return html_sequences\n",
    "    \n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b22da25-1f52-4c00-81b7-21514486cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(x_train, y_train, batch_size, tokenizer)\n",
    "test_generator = DataGenerator(x_test, y_test, batch_size, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa3604f-6c37-443a-9332-f53cb28db259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93318d47-03f3-4035-8333-430109465e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pre-trained MobileBERT model and tokenizer\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"google/mobilebert-uncased\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mobilebert-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "003b9f4d-17e4-4883-81cd-0ab05abb5b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 700, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(html_max_length,))\n",
    "\n",
    "embedding = Embedding(input_dim=tokenizer_vocab_size, input_length=html_max_length, output_dim=embed_dimension)(input)\n",
    "embedding = tf.expand_dims(embedding, -1)  # Add channel dimension\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59de42ba-8431-4826-9482-ddd0b0c2c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_3 = Conv2D(num_filters, (3, embed_dimension))(embedding)\n",
    "conv_3 = BatchNormalization()(conv_3)  # Batch normalization after convolution\n",
    "conv_3 = Activation('relu')(conv_3) \n",
    "conv_3 = MaxPooling2D((2, 1), strides=(2, 1))(conv_3)\n",
    "# h = 5\n",
    "conv_5 = Conv2D(num_filters, (5, embed_dimension), activation='relu')(embedding)\n",
    "conv_5 = BatchNormalization()(conv_5)  # Batch normalization after convolution\n",
    "conv_5 = Activation('relu')(conv_5) \n",
    "conv_5 = MaxPooling2D((2, 1), strides=(2, 1))(conv_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5872fce4-fbb6-4e95-a847-82d935df15c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 178432)\n"
     ]
    }
   ],
   "source": [
    "#concatenate all convolutional layer outputs\n",
    "concatenated = Concatenate(axis=1)([conv_3,conv_5])\n",
    "flattened = Flatten()(concatenated)\n",
    "print(flattened.shape)\n",
    "\n",
    "# feed concatenated conv layers to fully conected layer\n",
    "dense = Dense(512,activation='relu',kernel_regularizer=l2(0.01))(flattened)\n",
    "dropout = Dropout(0.5)(dense)\n",
    "dropout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93047df8-ea13-41ef-8801-7204679279a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[178432,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# feed concatenated conv layers to fully conected layer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dense \u001b[38;5;241m=\u001b[39m \u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mkernel_regularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m dropout \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.5\u001b[39m)(dense)\n\u001b[0;32m      4\u001b[0m dropout\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\backend.py:1920\u001b[0m, in \u001b[0;36mRandomGenerator.random_uniform\u001b[1;34m(self, shape, minval, maxval, dtype)\u001b[0m\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator:\n\u001b[0;32m   1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generator\u001b[38;5;241m.\u001b[39muniform(\n\u001b[0;32m   1919\u001b[0m       shape\u001b[38;5;241m=\u001b[39mshape, minval\u001b[38;5;241m=\u001b[39mminval, maxval\u001b[38;5;241m=\u001b[39mmaxval, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m-> 1920\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1921\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1922\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_legacy_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[178432,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:RandomUniform]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8a0a5-ff93-4158-9511-f9c308ad28a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe1d682-17d4-437e-b92c-804336478060",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = Dense(2, activation='softmax')(dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb98be-19a0-4a88-9ea4-4a1e494ffd33",
   "metadata": {},
   "source": [
    "# MobileBERT implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65aa1e6b-48a7-47cf-aac3-3b82b5725fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = [elem[0] for elem in training_data]\n",
    "# labels = [elem[1] for elem in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4873e9-9f95-4db6-9cfc-08b051874adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e764c537-910e-4067-af38-1a00f384dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/mobilebert-uncased\")\n",
    "\n",
    "# Tokenize the texts\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=html_max_length, return_tensors=\"tf\")\n",
    "\n",
    "# Tokenize your dataset\n",
    "inputs = tokenize_function(x_train)\n",
    "test_inputs = tokenize_function(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "703616ed-5119-4849-b3ce-bea0ad143e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TensorFlow Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(inputs), y_train))\n",
    "dataset_test =  tf.data.Dataset.from_tensor_slices((dict(test_inputs), y_test))\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "train_dataset = dataset.shuffle(len(x_train)).batch(batch_size)\n",
    "test_dataset = dataset.shuffle(len(x_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84a9b7dc-3e8e-4e2a-acd6-80f17cc33490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMobileBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFMobileBertForSequenceClassification were not initialized from the model checkpoint at google/mobilebert-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load MobileBERT for classification\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"google/mobilebert-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7879edc5-74e6-4ca5-b6d8-e18e63d04531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = ModelCheckpoint('mobileBERT_alpha_2k.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b8b3b71-a077-4ae1-b57c-87aa23869e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb5af9e6-bb44-4b55-9af5-303595b80ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "200/200 [==============================] - 64s 320ms/step - loss: 34.6081 - accuracy: 0.6350 - val_loss: 1.1917 - val_accuracy: 0.9381\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 66s 331ms/step - loss: 0.5832 - accuracy: 0.9481 - val_loss: 0.2821 - val_accuracy: 0.9594\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 67s 334ms/step - loss: 0.2571 - accuracy: 0.9550 - val_loss: 0.1831 - val_accuracy: 0.9725\n",
      "Epoch 4/10\n",
      " 11/200 [>.............................] - ETA: 37s - loss: 0.3669 - accuracy: 0.9545"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42a964d4-c4d1-49c1-8d7e-2dfe6fc1d17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[  101, 22555,  6979,  2497, 22555,  3348,   102,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0]])>, 'token_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_function([\"\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
