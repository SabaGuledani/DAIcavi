{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e325ae-d79a-4ba3-bfac-ae37a8ede766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import sqlite3\n",
    "import jsonlines\n",
    "from collections import Counter\n",
    "from unicodedata import normalize\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Embedding\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e669f84-aa46-46e8-b05c-c93cc9209d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_vocab_size = 20000\n",
    "html_max_length = 1000\n",
    "embed_dimension = 4\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7738e6-a2cb-416f-a7c6-5a8f8a2a7242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_and_average(seq_list):\n",
    "    lengths = [len(doc.ids) for doc in seq_list]\n",
    "    lengths.sort()\n",
    "    lengths_avg = sum(lengths)/len(lengths)\n",
    "    median = lengths[len(lengths)//2]\n",
    "    print(f\"median: {median}\")\n",
    "    print(f\"average length: {int(lengths_avg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4017d5b4-feee-4618-a4f5-9a16944b2af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Load the list from the pickle file\n",
    "with open('htmls_parsed_text_list.pkl', 'rb') as f:\n",
    "    loaded_html_documents = pickle.load(f)\n",
    "    print(\"list loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98bc21c-13e0-4b5e-8164-1aede2948b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9303a053-3c6a-4d6d-b366-8294ba867582",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train_from_iterator(loaded_html_documents,\n",
    "                              vocab_size=tokenizer_vocab_size, \n",
    "                              min_frequency=2,\n",
    "                              special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87017445-76b5-4eb1-8c5c-b218b7933dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = \n",
    "# encoded_html_docs = [tokenizer.encode(doc) for doc in loaded_html_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd681c1-1887-4a0a-a62b-24b4940b7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(path):\n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    tokenizer.save_model(path)\n",
    "    print(\"tokenizer saved successfully\")\n",
    "\n",
    "# Load model\n",
    "def load_tokenizer(path):\n",
    "    tokenizer = ByteLevelBPETokenizer(f\"{path}/vocab.json\", f\"{path}/merges.txt\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3b4e68-cff2-4ca5-9804-a1fa88d95c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer saved successfully\n"
     ]
    }
   ],
   "source": [
    "save_tokenizer(\"bpe_tokenizer_20k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b22da25-1f52-4c00-81b7-21514486cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_and_pad_sequence(sequence, max_length, pad_token=\"<pad>\"):\n",
    "    pad_token_id = 3\n",
    "\n",
    "   \n",
    "    token_ids =  tokenizer.encode(sequence).ids\n",
    "    if len(token_ids) < max_length:\n",
    "        # Pad with <pad> token\n",
    "        token_ids.extend([pad_token_id] * (max_length - len(token_ids)))\n",
    "    return token_ids[:max_length]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "faa3604f-6c37-443a-9332-f53cb28db259",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_padded_html_docs = [encode_and_pad_sequence(doc,html_max_length) for doc in loaded_html_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "93318d47-03f3-4035-8333-430109465e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_html_docs = None\n",
    "loaded_html_documents = None\n",
    "del encoded_html_docs\n",
    "del loaded_html_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db06be-1561-4a27-a486-c89ec8cadb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(html_max_length,))\n",
    "\n",
    "embedding = Embedding(input_dim=tokenizer_vocab_size, input_length=html_max_length, output_dim=embed_dimension)(input)\n",
    "print(embedding.shape)\n",
    "\n",
    "output_layer = Dense(2, activation='softmax')(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac2c34-86bf-4ac0-a69a-3044734f95ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, htmls, labels, batch_size, char_tokenizer, word_tokenizer):\n",
    "        self.htmls = htmls\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.char_tokenizer = char_tokenizer\n",
    "        self.indexes = np.arange(len(htmls))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.htmls) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_htmls = [self.htmls[i] for i in batch_indexes]\n",
    "        batch_labels = [self.labels[i] for i in batch_indexes]\n",
    "        \n",
    "        char_input_data = self._preprocess_char_input(batch_htmls)\n",
    "        word_input_data = self._preprocess_word_input(batch_htmls)\n",
    "        word_char_input_data = self._preprocess_word_char_input(batch_htmls)\n",
    "        \n",
    "        return [char_input_data, word_input_data, word_char_input_data], np.array(batch_labels)\n",
    "    \n",
    "    def _preprocess_char_input(self, htmls):\n",
    "        char_sequences = self.char_tokenizer.texts_to_sequences(htmls)\n",
    "        return pad_sequences(char_sequences, maxlen=MAX_URL_LEN_CHAR, padding='post',truncating='post')\n",
    "    \n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ffd6fc-1d3f-4271-a942-904c624bec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764c537-910e-4067-af38-1a00f384dde0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703616ed-5119-4849-b3ce-bea0ad143e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9b7dc-3e8e-4e2a-acd6-80f17cc33490",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
