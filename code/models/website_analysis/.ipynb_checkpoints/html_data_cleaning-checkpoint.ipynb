{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49bd31f1-2e2f-4897-ae07-b1048b7d2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import sqlite3\n",
    "import jsonlines\n",
    "from collections import Counter\n",
    "from unicodedata import normalize\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Flatten\n",
    "# from tensorflow.keras.models import Model, load_model\n",
    "# from tensorflow.keras.regularizers import l1, l2\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.utils import Sequence\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "# from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41cb9fb-03bc-4d4f-9c17-2d0b05cdcb74",
   "metadata": {},
   "source": [
    "# split big jsonl files to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a34996e-6903-469c-9853-51943f843e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = os.listdir(r\"C:\\Users\\Saba\\Desktop\\Python\\AI CS hackathon btu\\datasets\\websites\\comprehensive_dataset\")\n",
    "def replace_unnecessary_symbols(text):\n",
    "    filters = '1234567890!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r\\b'\n",
    "    translate_dict = {c: \" \" for c in filters}\n",
    "    text = text.lower()\n",
    "    # Create a translation table\n",
    "    translation_table = str.maketrans(translate_dict)\n",
    "    html_translate_dict = {\"href\":\" \",\n",
    "                           \"src\": \" \",\n",
    "                           \" vc \":\" \",\n",
    "                           \"jpg\":\" \",\n",
    "                          \" en \":\" \",\n",
    "                          \"thumb\":\" \",\n",
    "                          'title':\" \",\n",
    "                          \" com \": \" \",\n",
    "                          \" https \":\" \",\n",
    "                          \" ru \":\" \",\n",
    "                          \" net \": \" \",\n",
    "                          \"\\u00a9\":\" \",\n",
    "                          \" tags \":\" \",\n",
    "                          \" hd \":\" \",\n",
    "                          '\\u0308':\" \"}\n",
    "    # html_translation_table = str.maketrans(html_translate_dict)\n",
    "    # Use translate method\n",
    "    new_text = text.translate(translation_table)\n",
    "    for key in html_translate_dict:\n",
    "        new_text = new_text.replace(key,html_translate_dict[key])\n",
    "    new_text = re.sub(r'\\s+', ' ', new_text)\n",
    "    return new_text\n",
    "def save_json(filepath_to_save,json_index,dict,name):\n",
    "    with open(f'{filepath_to_save}/{json_index}_{name}_cleaned.json', 'w') as f:\n",
    "        json.dump(dict, f, indent=4)\n",
    "def save_to_chunks(name,file):\n",
    "    \n",
    "    chunks = pd.read_json(file, lines=True, chunksize=1)\n",
    "    num = 1\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            html_content = chunk[\"html\"].values[0]\n",
    "            soup  = BeautifulSoup(html_content, 'html')\n",
    "        except:\n",
    "            print(f\"could not parse row number {num}\")\n",
    "            continue\n",
    "        \n",
    "        for tag in soup(['script', 'style', 'img',\"input\",'iframe']):\n",
    "            tag.decompose()\n",
    "        for a_tag in soup.find_all('a'):\n",
    "            del a_tag['href']\n",
    "        for tag in soup.find_all(True):  # True selects all tags\n",
    "            if 'style' in tag.attrs:\n",
    "                del tag.attrs['style'] \n",
    "            if 'class' in tag.attrs:\n",
    "                del tag.attrs['class']\n",
    "            if 'target' in tag.attrs:\n",
    "                del tag.attrs['target']\n",
    "                \n",
    "        # we should get rid of tab and escape charaters, also weird symbols and replace duplicate spaces with one space\n",
    "        html_text = soup.get_text(separator=' ')\n",
    "        html_text = normalize('NFKD', html_text)\n",
    "        \n",
    "        html_text = replace_unnecessary_symbols(html_text)\n",
    "        \n",
    "        url = normalize('NFKD', chunk['url'].values[0])\n",
    "        label = chunk['label'].values[0]\n",
    "        \n",
    "        processed_json = {\n",
    "            \"url\":url,\n",
    "            \"label\":label,\n",
    "            \"html\":html_text\n",
    "        }\n",
    "        filepath_to_save = 'C:/Users/Saba/Desktop/Python/AI CS hackathon btu/datasets/websites/comprehensive_chunks_cleaned'\n",
    "        save_json(filepath_to_save,num,processed_json,name)\n",
    "        if num % 50 == 0:\n",
    "            print(f\"{num}/{20000}\", end='\\r')\n",
    "            if num % 1000 == 0:\n",
    "                break\n",
    "        num +=1\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b59d8994-8f4c-4194-925b-989e5860bd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Saba/Desktop/Python/AI CS hackathon btu/datasets/websites/comprehensive_dataset/benign_OWS_URL_HTML_DS_part_1.jsonl\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "file = \"C:/Users/Saba/Desktop/Python/AI CS hackathon btu/datasets/websites/comprehensive_dataset/\"+datasets[1]\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcaa3a64-aeb1-4669-9962-54cd9fe410be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/20000\r"
     ]
    }
   ],
   "source": [
    "save_to_chunks(\"benign\",file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20dc972-15d5-428f-9fcb-e9ef7229ccbb",
   "metadata": {},
   "source": [
    "# Load chunks of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "988f0545-abf3-4bb0-b917-c1f3487c7f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunk_to_dataframe(chunk_index):\n",
    "    chunks_filepath = \"C:/Users/Saba/Desktop/Python/AI CS hackathon btu/datasets/websites/comprehensive_chunks_parsed/\"\n",
    "    chunks = os.listdir(chunks_filepath)\n",
    "    chunk_fullpath = chunks_filepath+chunks[chunk_index]\n",
    "    df = pd.read_json(chunk_fullpath)\n",
    "    # df.drop(columns=[\"sublabel\",'crawled','uid'],inplace=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ec85d60c-2b8a-4552-8908-962ebf9bb45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_filepath = \"C:/Users/Saba/Desktop/Python/AI CS hackathon btu/datasets/websites/comprehensive_chunks_parsed/\"\n",
    "chunks = os.listdir(chunks_filepath)\n",
    "chunks_length = len(chunks)\n",
    "# print(chunks_length)\n",
    "json_files_full = [chunks_filepath+chunk for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d22ba5-5f8e-4f70-9a72-3cd58620b4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
