{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf7d85f-bd9e-49ce-a642-0d868883a78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import requests\n",
    "import json\n",
    "from unicodedata import normalize\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "from tensorflow.keras.layers import Input, Dense,Concatenate, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, MaxPooling2D,Dropout, Add,Embedding\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d7a9837-f835-4446-b650-cbf71fe26bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39594d71-2a5c-415f-9a75-b27f907c710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_URL_LEN_CHAR = 100\n",
    "MAX_URL_LEN_WORD = 100\n",
    "MAX_WORD_LEN = 20\n",
    "int_to_class = {0: 'Adult', 1: 'Benign', 2: 'Malicious'}\n",
    "img_int_to_class = {0:'nude', 1:'safe', 2:'unsafe'}\n",
    "decode = lambda int_: int_to_class[int_]\n",
    "\n",
    "decode_img_class = lambda img_class_int: img_int_to_class[img_class_int]\n",
    "def text_to_word_sequence(\n",
    "    input_text,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "):\n",
    "    \n",
    "    if lower:\n",
    "        input_text = input_text.lower()\n",
    "\n",
    "    translate_dict = {c: split for c in filters}\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    input_text = input_text.translate(translate_map)\n",
    "\n",
    "    seq = input_text.split(split)\n",
    "    return [i for i in seq if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ac063b9-969e-4ca9-b1f6-63b0fcc62878",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./tokenizers/char_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    char_tokenizer = tokenizer_from_json(data)\n",
    "\n",
    "with open('./tokenizers/word_tokenizer.json') as f:\n",
    "    data = json.load(f)\n",
    "    word_tokenizer = tokenizer_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeec8a41-74e4-4316-b96f-0996c354fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Url_preprocessor():\n",
    "    def __init__(self, char_tokenizer, word_tokenizer):\n",
    "        self.char_tokenizer = char_tokenizer\n",
    "        self.word_tokenizer = word_tokenizer\n",
    "        \n",
    "    def _preprocess_char_input(self, url):\n",
    "        char_sequences = self.char_tokenizer.texts_to_sequences(url)\n",
    "        return pad_sequences(char_sequences, maxlen=MAX_URL_LEN_CHAR, padding='post',truncating='post')\n",
    "    \n",
    "    def _preprocess_word_input(self, url):\n",
    "        word_sequences = self.word_tokenizer.texts_to_sequences(url)\n",
    "        return pad_sequences(word_sequences, maxlen=MAX_URL_LEN_WORD, padding='post',truncating='post')\n",
    "    \n",
    "    def _preprocess_word_char_input(self, url):\n",
    "        sentences_splitted = [text_to_word_sequence(sentence) for sentence in url]\n",
    "    \n",
    "        for i in range(len(sentences_splitted)):\n",
    "            if len(sentences_splitted[i])>MAX_URL_LEN_CHAR:\n",
    "                sentences_splitted[i] = sentences_splitted[i][:MAX_URL_LEN_CHAR]\n",
    "            else:\n",
    "                while len(sentences_splitted[i])<MAX_URL_LEN_CHAR:\n",
    "                    sentences_splitted[i].append('<OOV>')\n",
    "            \n",
    "        words_splitted = [[word.split() for word in sentence] for sentence in sentences_splitted]\n",
    "    \n",
    "        word_char_sequences = []\n",
    "        \n",
    "        for sentence in words_splitted:\n",
    "            sentence_tokenized = []\n",
    "            for i in range(len(sentence)):\n",
    "                word = self.char_tokenizer.texts_to_sequences(sentence[i])\n",
    "                word_char_padded = pad_sequences(word,maxlen=MAX_WORD_LEN,padding='post',truncating='post')\n",
    "                sentence_tokenized.append(word_char_padded)\n",
    "            word_char_sequences.append(sentence_tokenized)\n",
    "        try:\n",
    "            word_char_sequences = np.array(word_char_sequences)\n",
    "        except:\n",
    "            print(url)\n",
    "        try:\n",
    "            word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n",
    "        except:\n",
    "            print(url)\n",
    "        return word_char_sequences\n",
    "\n",
    "    def preprocess(self,url):\n",
    "        url = [url]\n",
    "        char_input_data = self._preprocess_char_input(url)\n",
    "        word_input_data = self._preprocess_word_input(url)\n",
    "        word_char_input_data = self._preprocess_word_char_input(url)\n",
    "        \n",
    "        return [char_input_data, word_input_data, word_char_input_data]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7457a1cb-ce5b-4ea1-8316-939b749b7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_url(url):\n",
    "    url_preprocessor = Url_preprocessor(char_tokenizer,word_tokenizer)\n",
    "    url_to_predict = url_preprocessor.preprocess(url)\n",
    "    prediction = decode(model.predict(url_to_predict).argmax())\n",
    "    print(prediction)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6deadda0-f58b-4c3f-b499-97272f382bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name):\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9357471-ba14-4603-b0b6-b91a7b8415b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60db6a6e-64e6-42a0-9c3a-1e6f40dd6cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malicious\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Malicious'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_url('http://architecture.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7236f1-69cf-40b9-ab3e-464fd253e2d3",
   "metadata": {},
   "source": [
    "# MobileNet Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acae7fc9-c21e-4676-a009-9528a358f029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7deb477d-7cce-4012-84a0-e2e693077090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import matplotlib.image as mpimg\n",
    "mobilenet = load_model('mobilenet.h5')\n",
    "\n",
    "def read_img_src(url):\n",
    "    response = requests.get(url)\n",
    "    img = mpimg.imread(BytesIO(response.content), format='JPG')\n",
    "    # plt.imshow(img)\n",
    "    # plt.axis('off')  # Turn off axis\n",
    "    # plt.show()\n",
    "    return img\n",
    "\n",
    "def predict_img(url_src,model):\n",
    "    img = read_img_src(url_src)\n",
    "    resize = tf.image.resize(img, (128,128))\n",
    "    yhat = model.predict(np.expand_dims(resize/255, 0))\n",
    "    yhat = yhat.argmax()\n",
    "    print(f'predicted class is: {decode_img_class(yhat)}')\n",
    "    return decode_img_class(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d6540d-e01d-4468-b3f3-22561632d319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7878bf8b-0dc2-4909-b049-9cbde1785bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = read_img_src(\"https://scontent.xx.fbcdn.net/v/t1.15752-9/429102256_944359293332681_5837751611829704724_n.jpg?stp=dst-jpg_s851x315&_nc_cat=106&ccb=1-7&_nc_sid=5f2048&_nc_eui2=AeEUmA_6VukR6C4KfVVwkzNyvQWo3sseoky9Bajeyx6iTBsjBz-lbsriaLhMGT5HwunEAiqkeQ-CUYmu0ElPYxb-&_nc_ohc=aAvWhhMUzcMQ7kNvgGubamF&_nc_ad=z-m&_nc_cid=0&_nc_ht=scontent.xx&oh=03_Q7cD1QFbun4C4fQnvW7HP2RGPeTvN7OPlpFBmYN5Pt_WeX4pbQ&oe=669F28F4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c46bf50b-3e39-4611-b219-a37e3ac7a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = \"https://scontent.xx.fbcdn.net/v/t1.15752-9/429102256_944359293332681_5837751611829704724_n.jpg?stp=dst-jpg_s851x315&_nc_cat=106&ccb=1-7&_nc_sid=5f2048&_nc_eui2=AeEUmA_6VukR6C4KfVVwkzNyvQWo3sseoky9Bajeyx6iTBsjBz-lbsriaLhMGT5HwunEAiqkeQ-CUYmu0ElPYxb-&_nc_ohc=aAvWhhMUzcMQ7kNvgGubamF&_nc_ad=z-m&_nc_cid=0&_nc_ht=scontent.xx&oh=03_Q7cD1QFbun4C4fQnvW7HP2RGPeTvN7OPlpFBmYN5Pt_WeX4pbQ&oe=669F28F4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77b031b9-1f63-43d4-9ead-5b7595d50547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class is: safe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'safe'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_img(img,mobilenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196cddd8-d572-4639-82c6-1845e9057e53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
