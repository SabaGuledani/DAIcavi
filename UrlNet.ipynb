{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a38068-d7f9-4899-b84a-c113be987162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from unicodedata import normalize\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "from tensorflow.keras.layers import Input, Dense,Concatenate, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, MaxPooling2D,Dropout, Add,Embedding\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5a2a6d-88b5-41f5-a4a6-96646dd58c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_URL_LEN_CHAR = 150\n",
    "MAX_URL_LEN_WORD = 150\n",
    "MAX_WORD_LEN = 15\n",
    "k = 32\n",
    "num_filters = 256\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b8a87e-c42e-4ad0-8682-ce6688cbe99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('casino.db')\n",
    "\n",
    "# df = pd.read_sql('select * from web_pages',conn)\n",
    "# df.drop(columns=[\"id\",\"url\",\"html\",\"screenshot\"],inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76006942-f578-471a-88d1-4662440117cc",
   "metadata": {},
   "source": [
    "# Load dmoz and porn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6490f81-6890-4dd4-a271-e992a9a551a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1854441\n",
      "1822814\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/dmoz.csv')\n",
    "df_porn_casino = pd.read_csv('./porn_casino_links.csv')\n",
    "df_porn_casino = df_porn_casino[:200000]\n",
    "df_malicious = pd.read_csv('malicious_links.csv')\n",
    "df = pd.concat([df,df_malicious,df_porn_casino],ignore_index=True)\n",
    "\n",
    "print(len(df))\n",
    "#drop Nans\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "df['url'] = df['url'].apply(lambda word: normalize('NFKD', word))\n",
    "print(len(df))\n",
    "# shuffle the data\n",
    "df = df.sample(frac=1, random_state=44).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8204051-42c7-47e4-9f8e-419b401d49a8",
   "metadata": {},
   "source": [
    "# Load malicious links dataset and merge with main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d80e70-862a-405f-b04d-b8eb9d9f44d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classes = sorted(list(set(df.category.values)))\n",
    "num_classes = len(classes)\n",
    "print(num_classes)\n",
    "\n",
    "class_to_int = { word:i for i,word in enumerate(classes)}\n",
    "int_to_class = {i:word for i,word in enumerate(classes)}\n",
    "\n",
    "encode = lambda class_: class_to_int[class_]\n",
    "\n",
    "df['category'] = df['category'].apply(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbc03e50-9506-4778-b9f5-723b1e09df58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training: 1458251 for testing: 182281 for validating: 182282\n",
      "http://www.bckayaks.com 2\n"
     ]
    }
   ],
   "source": [
    "train_split = int(0.8 * len(df))\n",
    "x_train = df.url.values[:train_split]\n",
    "y_train = df.category.values[:train_split]\n",
    "\n",
    "x_test = df.url.values[train_split:train_split+ int(0.1 * len(df)) ]\n",
    "y_test =  df.category.values[train_split:train_split+ int(0.1 * len(df))]\n",
    "\n",
    "x_val = df.url.values[train_split+ int(0.1 * len(df)): ]\n",
    "y_val =  df.category.values[train_split+ int(0.1 * len(df)): ]\n",
    "print(\"for training: \" + str(len(x_train))+ \" for testing: \" + str(len(x_test))+ \" for validating: \" + str(len(x_val)))\n",
    "print(x_train[0],y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85721c7-357b-455d-8577-f8a55fdc6a47",
   "metadata": {},
   "source": [
    "# Tokenizers and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3173b-2739-48cf-81bc-819eb9845db7",
   "metadata": {},
   "source": [
    "## Character level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2706f935-edca-4632-9415-fbcf68300ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character tokenization\n",
    "char_tokenizer = Tokenizer(char_level=True, oov_token='<OOV>')\n",
    "char_tokenizer.fit_on_texts(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a4c253-ec28-4baa-af68-8fd7abdd7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_sequence(\n",
    "    input_text,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "):\n",
    "    \n",
    "    if lower:\n",
    "        input_text = input_text.lower()\n",
    "\n",
    "    translate_dict = {c: split for c in filters}\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    input_text = input_text.translate(translate_map)\n",
    "\n",
    "    seq = input_text.split(split)\n",
    "    return [i for i in seq if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab3535-0a29-4902-b327-35b19734fec8",
   "metadata": {},
   "source": [
    "### Word level tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290ba02-e7e2-4bde-bf56-bb469f3510aa",
   "metadata": {},
   "source": [
    "#### Calculate number of words which appear more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e5e0cd6-4971-4a0d-a59a-d149c92327d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_calculation():\n",
    "    word_sequences = [text_to_word_sequence(sentence) for sentence in x_train]\n",
    "    all_words_concat = [item for sublist in word_sequences for item in sublist]\n",
    "    freq_dict_words = Counter(all_words_concat)\n",
    "    \n",
    "    low_freq_words_count = 0\n",
    "    for key in freq_dict_words:\n",
    "        if freq_dict_words[key] == 1:\n",
    "            low_freq_words_count += 1\n",
    "    \n",
    "    high_freq_word_count = int(len(freq_dict_words) * (1-(low_freq_words_count/len(freq_dict_words))))\n",
    "    print(high_freq_word_count)\n",
    "    return high_freq_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6287bae-e6a6-44e0-a536-7dc684628db2",
   "metadata": {},
   "source": [
    "#### Url into sequence of word ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ddcdf90-bbc5-42a9-96fc-2b1d0dd53d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188656\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "188656"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_freq_word_count = frequency_calculation()\n",
    "high_freq_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e575df-1a1b-4144-8749-8bdbe5128187",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer(oov_token='<OOV>',num_words=high_freq_word_count)\n",
    "word_tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8fb5ec-667d-45c1-ac9b-2b3546b711b8",
   "metadata": {},
   "source": [
    "## define DataGenerator for preprocessing(tokenizing and padding) inputs on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc050e31-9d4a-42c8-a76b-39ff17fd66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, urls, labels, batch_size, char_tokenizer, word_tokenizer):\n",
    "        self.urls = urls\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.char_tokenizer = char_tokenizer\n",
    "        self.word_tokenizer = word_tokenizer\n",
    "        self.indexes = np.arange(len(urls))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.urls) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_urls = [self.urls[i] for i in batch_indexes]\n",
    "        batch_labels = [self.labels[i] for i in batch_indexes]\n",
    "        \n",
    "        char_input_data = self._preprocess_char_input(batch_urls)\n",
    "        word_input_data = self._preprocess_word_input(batch_urls)\n",
    "        word_char_input_data = self._preprocess_word_char_input(batch_urls)\n",
    "        \n",
    "        return [char_input_data, word_input_data, word_char_input_data], np.array(batch_labels)\n",
    "    \n",
    "    def _preprocess_char_input(self, urls):\n",
    "        char_sequences = self.char_tokenizer.texts_to_sequences(urls)\n",
    "        return pad_sequences(char_sequences, maxlen=MAX_URL_LEN_CHAR, padding='post',truncating='post')\n",
    "    \n",
    "    def _preprocess_word_input(self, urls):\n",
    "        word_sequences = self.word_tokenizer.texts_to_sequences(urls)\n",
    "        return pad_sequences(word_sequences, maxlen=MAX_URL_LEN_WORD, padding='post',truncating='post')\n",
    "    \n",
    "    def _preprocess_word_char_input(self, urls):\n",
    "        sentences_splitted = [text_to_word_sequence(sentence) for sentence in urls]\n",
    "    \n",
    "        for i in range(len(sentences_splitted)):\n",
    "            if len(sentences_splitted[i])>MAX_URL_LEN_CHAR:\n",
    "                sentences_splitted[i] = sentences_splitted[i][:MAX_URL_LEN_CHAR]\n",
    "            else:\n",
    "                while len(sentences_splitted[i])<MAX_URL_LEN_CHAR:\n",
    "                    sentences_splitted[i].append('<OOV>')\n",
    "            \n",
    "        words_splitted = [[word.split() for word in sentence] for sentence in sentences_splitted]\n",
    "\n",
    "        word_char_sequences = []\n",
    "        \n",
    "        for sentence in words_splitted:\n",
    "            sentence_tokenized = []\n",
    "            for i in range(len(sentence)):\n",
    "                word = self.char_tokenizer.texts_to_sequences(sentence[i])\n",
    "                word_char_padded = pad_sequences(word,maxlen=MAX_WORD_LEN,padding='post',truncating='post')\n",
    "                sentence_tokenized.append(word_char_padded)\n",
    "            word_char_sequences.append(sentence_tokenized)\n",
    "        try:\n",
    "            word_char_sequences = np.array(word_char_sequences)\n",
    "        except:\n",
    "            print(urls)\n",
    "        try:\n",
    "            word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n",
    "        except:\n",
    "            print(urls)\n",
    "        return word_char_sequences\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dae70f16-47ae-42b8-8bf1-9fdd3e79e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(x_train, y_train, batch_size, char_tokenizer, word_tokenizer)\n",
    "test_generator = DataGenerator(x_test, y_test, batch_size, char_tokenizer, word_tokenizer)\n",
    "val_generator = DataGenerator(x_val, y_val, batch_size, char_tokenizer, word_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e796c-0b9f-40c4-a47b-841d60632495",
   "metadata": {},
   "source": [
    "# Character level CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74be879-6950-4b58-8cb7-476c8a46a126",
   "metadata": {},
   "source": [
    "### Character embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61540b00-775d-454d-bc19-0c8d15870246",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_input = Input((MAX_URL_LEN_CHAR,))\n",
    "\n",
    "# Embedding layer\n",
    "char_embedding = Embedding(input_dim=len(char_tokenizer.word_index) + 1, output_dim=k, input_length=MAX_URL_LEN_CHAR)(char_input)\n",
    "char_embedding = tf.expand_dims(char_embedding, -1)  # Add channel dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc5240-1a1f-4970-bc31-daf77f09f632",
   "metadata": {},
   "source": [
    "# CNN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09304aab-616f-4e73-b7c1-94c3afa40c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution and pooling for character-level\n",
    "# h = 3\n",
    "conv_3_char = Conv2D(num_filters, (3, k), activation='relu')(char_embedding)\n",
    "conv_3_char = BatchNormalization()(conv_3_char)  # Batch normalization after convolution\n",
    "conv_3_char = Activation('relu')(conv_3_char) \n",
    "conv_3_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_3_char)\n",
    "# h = 4\n",
    "conv_4_char = Conv2D(num_filters, (4, k), activation='relu')(char_embedding)\n",
    "conv_4_char = BatchNormalization()(conv_4_char)  # Batch normalization after convolution\n",
    "conv_4_char = Activation('relu')(conv_4_char) \n",
    "conv_4_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_4_char)\n",
    "# h = 5\n",
    "conv_5_char = Conv2D(num_filters, (5, k), activation='relu')(char_embedding)\n",
    "conv_5_char = BatchNormalization()(conv_5_char)  # Batch normalization after convolution\n",
    "conv_5_char = Activation('relu')(conv_5_char) \n",
    "conv_5_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_5_char)\n",
    "# h = 6\n",
    "conv_6_char = Conv2D(num_filters, (6, k), activation='relu')(char_embedding)\n",
    "conv_6_char = BatchNormalization()(conv_6_char)  # Batch normalization after convolution\n",
    "conv_6_char = Activation('relu')(conv_6_char) \n",
    "conv_6_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_6_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327cf171-b6ac-469d-96c0-1a0cfc727537",
   "metadata": {},
   "source": [
    "### Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83263576-ddc6-4ee2-9d81-6274fc249abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all convolutional layer outputs\n",
    "concatenated = Concatenate(axis=1)([conv_3_char,conv_4_char,conv_5_char,conv_6_char])\n",
    "flattened = Flatten()(concatenated)\n",
    "\n",
    "# feed concatenated conv layers to fully conected layer\n",
    "dense_char = Dense(512,activation='relu')(flattened)\n",
    "dropout = Dropout(0.5)(dense_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de878ba-24e2-4a1f-971f-707e1217e7ee",
   "metadata": {},
   "source": [
    "# Word-level Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c666a-004e-4a82-af38-12b26fd99b78",
   "metadata": {},
   "source": [
    "##### reduce the size of word_index to reduce embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e4eb4cf-39dc-4a04-8824-236b699295b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_words = dict(list(word_tokenizer.word_index.items())[:high_freq_word_count])\n",
    "most_freq_words[word_tokenizer.oov_token] = high_freq_word_count + 1  # Add OOV token index manually\n",
    "\n",
    "# Step 3: Use the length of this new word index for the embedding layer\n",
    "input_dim = len(most_freq_words) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed5e13-8e51-49e9-9fc0-8a6bd18f1733",
   "metadata": {},
   "source": [
    "##### word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee2a87e9-0061-4b28-89d8-f587b1072ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 150, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_input = Input(shape=(MAX_URL_LEN_WORD,))\n",
    "\n",
    "# Embedding layer\n",
    "word_embedding = Embedding(input_dim=input_dim, output_dim=k, input_length=MAX_URL_LEN_WORD)(word_input)\n",
    "\n",
    "word_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c36b41-0923-4fd9-a689-5647ac5d3035",
   "metadata": {},
   "source": [
    "##### char embedding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f40d8bc1-9d92-4348-ad7f-951306206316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 150, 15, 32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_char_input = Input(shape=(MAX_URL_LEN_CHAR,MAX_WORD_LEN))\n",
    "\n",
    "# Embedding layer\n",
    "word_char_embedding = Embedding(input_dim=len(char_tokenizer.word_index) + 1, output_dim=k, input_length=MAX_URL_LEN_CHAR)(word_char_input)\n",
    "word_char_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c0756-aee1-4a54-a1f6-9f1c653fa517",
   "metadata": {},
   "source": [
    "##### Sum over characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39bcff9a-b91e-4115-aa68-55567725db04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 150, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_layer = tf.keras.layers.Lambda(lambda x: tf.math.reduce_sum(x, axis=2))(word_char_embedding)\n",
    "pooled_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e6593-44e7-457a-8518-d4053b1e95d7",
   "metadata": {},
   "source": [
    "#### Element-wise Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70647042-c576-4d73-a11a-ef44ed6f5396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 150, 32, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition_layer = Add()([pooled_layer, word_embedding])\n",
    "addition_layer = tf.expand_dims(addition_layer,-1)\n",
    "\n",
    "addition_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315cd81-7ce6-4ba7-a08c-720deaf0e572",
   "metadata": {},
   "source": [
    "## Word-level CNN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f273801-80ae-4f8b-a88b-f852554a5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution and pooling for word-level\n",
    "# h = 3\n",
    "conv_3_word = Conv2D(num_filters, (3, k))(addition_layer)\n",
    "conv_3_word = BatchNormalization()(conv_3_word)  # Batch normalization after convolution\n",
    "conv_3_word = Activation('relu')(conv_3_word) \n",
    "conv_3_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_3_word)\n",
    "# h = 4\n",
    "conv_4_word = Conv2D(num_filters, (4, k), activation='relu')(addition_layer)\n",
    "conv_4_word = BatchNormalization()(conv_4_word)  # Batch normalization after convolution\n",
    "conv_4_word = Activation('relu')(conv_4_word) \n",
    "conv_4_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_4_word)\n",
    "# h = 5\n",
    "conv_5_word = Conv2D(num_filters, (5, k), activation='relu')(addition_layer)\n",
    "conv_5_word = BatchNormalization()(conv_5_word)  # Batch normalization after convolution\n",
    "conv_5_word = Activation('relu')(conv_5_word) \n",
    "conv_5_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_5_word)\n",
    "# h = 6\n",
    "conv_6_word = Conv2D(num_filters, (6, k), activation='relu')(addition_layer)\n",
    "conv_6_word = BatchNormalization()(conv_6_word)  # Batch normalization after convolution\n",
    "conv_6_word = Activation('relu')(conv_6_word) \n",
    "conv_6_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_6_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a7e21-a6da-4370-90fe-ea4630a8b8a1",
   "metadata": {},
   "source": [
    "## Word level fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46f6d25b-89f8-480c-b4fd-b7a5caadf213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate all convolutional layer outputs\n",
    "concatenated_word = Concatenate(axis=1)([conv_3_word,conv_4_word,conv_5_word,conv_6_word])\n",
    "flattened_word = Flatten()(concatenated_word)\n",
    "\n",
    "# feed concatenated conv layers to fully conected layer\n",
    "dense_word = Dense(512,activation='relu')(flattened_word)\n",
    "dropout_word = Dropout(0.5)(dense_word)\n",
    "dropout_word.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e3625-ac35-42be-9bf5-c1b357c64130",
   "metadata": {},
   "source": [
    "## concatenate outputs of char-level and word-level blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "909e1e11-355e-4581-ace8-e352b24d5304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1024])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenate_all = Concatenate()([dropout,dropout_word])\n",
    "concatenate_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0051b07-0774-4c98-8476-e3639aff5abf",
   "metadata": {},
   "source": [
    "# last fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f3a8963-9f6d-4351-8881-52af23d07ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_1 = Dense(512,activation='relu')(concatenate_all)\n",
    "dropout_fc_1 = Dropout(0.5)(fc_1)\n",
    "\n",
    "fc_2 = Dense(256,activation='relu')(dropout_fc_1)\n",
    "dropout_fc_2 = Dropout(0.5)(fc_2)\n",
    "\n",
    "fc_3 = Dense(128,activation='relu')(dropout_fc_2)\n",
    "dropout_fc_3 = Dropout(0.5)(fc_3)\n",
    "\n",
    "# output layer with softmax\n",
    "output = Dense(num_classes, activation='softmax')(dropout_fc_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74a465-0c35-4c17-9b50-0044de8810dc",
   "metadata": {},
   "source": [
    "# Model compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "365d22b8-0c38-4644-89f9-55e6e6fb38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce493f05-7754-4bbb-aa90-0b6a4ecb94f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 150, 15)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 150, 15, 32)  3040        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 150, 32)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 32)      6037024     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 150, 32)      3040        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 150, 32)      0           lambda[0][0]                     \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)     (None, 150, 32, 1)   0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (None, 150, 32, 1)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 148, 1, 256)  24832       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 147, 1, 256)  33024       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 146, 1, 256)  41216       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 145, 1, 256)  49408       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 148, 1, 256)  24832       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 147, 1, 256)  33024       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 146, 1, 256)  41216       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 145, 1, 256)  49408       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 148, 1, 256)  1024        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 147, 1, 256)  1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 146, 1, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 145, 1, 256)  1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 148, 1, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 147, 1, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 146, 1, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 145, 1, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 148, 1, 256)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 147, 1, 256)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 146, 1, 256)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 145, 1, 256)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 148, 1, 256)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 147, 1, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 146, 1, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 145, 1, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 74, 1, 256)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 73, 1, 256)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 73, 1, 256)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 72, 1, 256)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 74, 1, 256)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 73, 1, 256)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 73, 1, 256)   0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 72, 1, 256)   0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 292, 1, 256)  0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 292, 1, 256)  0           max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 74752)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 74752)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          38273536    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          38273536    flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1024)         0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          524800      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          131328      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          32896       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           2064        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 83,586,416\n",
      "Trainable params: 83,582,320\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[char_input, word_input, word_char_input],outputs=output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce833b10-8bff-40e0-8ccd-b146fe9f94ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "45571/45571 [==============================] - 2631s 58ms/step - loss: 1.7831 - accuracy: 0.4628\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "Epoch 2/8\n",
      "45571/45571 [==============================] - 2618s 57ms/step - loss: 1.6577 - accuracy: 0.5033\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "Epoch 3/8\n",
      " 7967/45571 [====>.........................] - ETA: 35:56 - loss: 1.6191 - accuracy: 0.5158"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# [char_input_data, word_input_data, word_char_input_data],\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# y_train,\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\engine\\training.py:1189\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1187\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs  \u001b[38;5;66;03m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1189\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1191\u001b[0m   \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\callbacks.py:435\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 435\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\callbacks.py:295\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    293\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 295\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    297\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(hook))\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\callbacks.py:315\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    312\u001b[0m   batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    313\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    318\u001b[0m   end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\callbacks.py:353\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    352\u001b[0m   hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 353\u001b[0m   \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    356\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\callbacks.py:1028\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1028\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\callbacks.py:1100\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1096\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1099\u001b[0m   \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1100\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\utils\\tf_utils.py:516\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    514\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t  \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\utils\\tf_utils.py:512\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    511\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 512\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m    514\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1094\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1094\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1060\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1059\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m     six\u001b[38;5;241m.\u001b[39mraise_from(core\u001b[38;5;241m.\u001b[39m_status_to_exception(e\u001b[38;5;241m.\u001b[39mcode, e\u001b[38;5;241m.\u001b[39mmessage), \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    training_generator,\n",
    "    # [char_input_data, word_input_data, word_char_input_data],\n",
    "    # y_train,\n",
    "    validation_data=val_generator,\n",
    "    epochs=8,\n",
    "    validation_freq=4,\n",
    "    callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00d49bda-5910-4ec9-ba97-8ec71d9471db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('training_ep3_50acc.h5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3bc2a9-cb9c-48df-8d62-44c13ff885be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
