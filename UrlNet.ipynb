{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0a38068-d7f9-4899-b84a-c113be987162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "from tensorflow.keras.layers import Input, Dense,Concatenate, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, MaxPooling2D,Dropout, Add,Embedding\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5a2a6d-88b5-41f5-a4a6-96646dd58c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_URL_LEN_CHAR = 150\n",
    "MAX_URL_LEN_WORD = 150\n",
    "MAX_WORD_LEN = 20\n",
    "k = 32\n",
    "num_filters = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b8a87e-c42e-4ad0-8682-ce6688cbe99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('casino.db')\n",
    "\n",
    "# df = pd.read_sql('select * from web_pages',conn)\n",
    "# df.drop(columns=[\"id\",\"url\",\"html\",\"screenshot\"],inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76006942-f578-471a-88d1-4662440117cc",
   "metadata": {},
   "source": [
    "# Load dmoz dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c05dd83b-4d78-4220-9b99-4afaae162435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562977\n",
      "1562974\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/dmoz.csv')\n",
    "print(len(df))\n",
    "#drop Nans\n",
    "df = df.dropna()\n",
    "print(len(df))\n",
    "\n",
    "# shuffle the data\n",
    "df = df.sample(frac=1, random_state=44).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d463cdd-180c-4054-b687-edbba76e146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(list(set(df.category.values)))\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbc03e50-9506-4778-b9f5-723b1e09df58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training: 1250379 for testing: 312595\n",
      "http://www.newadvent.org/cathen/11535a.htm Society\n"
     ]
    }
   ],
   "source": [
    "train_split = int(0.8 * len(df))\n",
    "x_train = df.url.values[:train_split]\n",
    "y_train = df.category.values[:train_split]\n",
    "\n",
    "x_test = df.url.values[train_split: ]\n",
    "y_test =  df.category.values[train_split:]\n",
    "\n",
    "print(\"for training: \" + str(len(x_train))+ \" for testing: \" + str(len(x_test)))\n",
    "print(x_train[0],y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85721c7-357b-455d-8577-f8a55fdc6a47",
   "metadata": {},
   "source": [
    "# Tokenizers and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3173b-2739-48cf-81bc-819eb9845db7",
   "metadata": {},
   "source": [
    "## Character level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2706f935-edca-4632-9415-fbcf68300ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character tokenization\n",
    "char_tokenizer = Tokenizer(char_level=True, oov_token='<OOV>')\n",
    "char_tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "def char_tokenize(batch):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67a4c253-ec28-4baa-af68-8fd7abdd7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_sequence(\n",
    "    input_text,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "):\n",
    "    \n",
    "    if lower:\n",
    "        input_text = input_text.lower()\n",
    "\n",
    "    translate_dict = {c: split for c in filters}\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    input_text = input_text.translate(translate_map)\n",
    "\n",
    "    seq = input_text.split(split)\n",
    "    return [i for i in seq if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab3535-0a29-4902-b327-35b19734fec8",
   "metadata": {},
   "source": [
    "### Word level tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290ba02-e7e2-4bde-bf56-bb469f3510aa",
   "metadata": {},
   "source": [
    "#### Calculate number of words which appear more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e5e0cd6-4971-4a0d-a59a-d149c92327d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_calculation():\n",
    "    word_sequences = [text_to_word_sequence(sentence) for sentence in x_train]\n",
    "    all_words_concat = [item for sublist in word_sequences for item in sublist]\n",
    "    freq_dict_words = Counter(all_words_concat)\n",
    "    \n",
    "    low_freq_words_count = 0\n",
    "    for key in freq_dict_words:\n",
    "        if freq_dict_words[key] == 1:\n",
    "            low_freq_words_count += 1\n",
    "    \n",
    "    high_freq_word_count = int(len(freq_dict_words) * (1-(low_freq_words_count/len(freq_dict_words))))\n",
    "    print(high_freq_word_count)\n",
    "    return high_freq_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6287bae-e6a6-44e0-a536-7dc684628db2",
   "metadata": {},
   "source": [
    "#### Url into sequence of word ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bda8eced-b077-4dcb-b9f4-e9684b333159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168710\n"
     ]
    }
   ],
   "source": [
    "high_freq_word_count = frequency_calculation()\n",
    "word_tokenizer = Tokenizer(oov_token='<OOV>',num_words=high_freq_word_count)\n",
    "word_tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "def word_tokenize(batch):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5256ed-e00c-42dc-a85b-4d876e62fd8f",
   "metadata": {},
   "source": [
    "#### url into sequence of words in char ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67ae8565-fda0-44be-96f0-b8a428b815ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_char_tokenize(batch):\n",
    "    sentences_splitted = [text_to_word_sequence(sentence) for sentence in batch]\n",
    "    \n",
    "    for i in range(len(sentences_splitted)):\n",
    "        while len(sentences_splitted[i])<150:\n",
    "            sentences_splitted[i].append('<PAD>')\n",
    "    \n",
    "    words_splitted = [[word.split() for word in sentence] for sentence in sentences_splitted]\n",
    "    \n",
    "    del sentences_splitted\n",
    "    \n",
    "    word_char_sequences = []\n",
    "    \n",
    "    for sentence in words_splitted:\n",
    "        sentence_tokenized = []\n",
    "        for i in range(len(sentence)):\n",
    "            word = char_tokenizer.texts_to_sequences(sentence[i])\n",
    "            word_char_padded = pad_sequences(word,maxlen=MAX_WORD_LEN,padding='post')\n",
    "            sentence_tokenized.append(word_char_padded)\n",
    "        word_char_sequences.append(sentence_tokenized)\n",
    "    \n",
    "    del words_splitted\n",
    "    word_char_sequences = np.array(word_char_sequences)\n",
    "    word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n",
    "    \n",
    "    return word_char_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8fb5ec-667d-45c1-ac9b-2b3546b711b8",
   "metadata": {},
   "source": [
    "## define DataGenerator for preprocessing(tokenizing and padding) inputs on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc050e31-9d4a-42c8-a76b-39ff17fd66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, urls, labels, batch_size, char_tokenizer, word_tokenizer):\n",
    "        self.urls = urls\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.char_tokenizer = char_tokenizer\n",
    "        self.word_tokenizer = word_tokenizer\n",
    "        self.indexes = np.arange(len(urls))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.urls) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_urls = [self.urls[i] for i in batch_indexes]\n",
    "        batch_labels = [self.labels[i] for i in batch_indexes]\n",
    "        \n",
    "        char_input_data = self._preprocess_char_input(batch_urls)\n",
    "        word_input_data = self._preprocess_word_input(batch_urls)\n",
    "        word_char_input_data = self._preprocess_word_char_input(batch_urls)\n",
    "        \n",
    "        return [char_input_data, word_input_data, word_char_input_data], np.array(batch_labels)\n",
    "    \n",
    "    def _preprocess_char_input(self, urls):\n",
    "        char_sequences = self.char_tokenizer.texts_to_sequences(urls)\n",
    "        return pad_sequences(char_sequences, maxlen=MAX_URL_LEN_CHAR, padding='post')\n",
    "    \n",
    "    def _preprocess_word_input(self, urls):\n",
    "        word_sequences = self.word_tokenizer.texts_to_sequences(urls)\n",
    "        return pad_sequences(word_sequences, maxlen=MAX_URL_LEN_WORD, padding='post')\n",
    "    \n",
    "    def _preprocess_word_char_input(self, urls):\n",
    "        sentences_splitted = [text_to_word_sequence(sentence) for sentence in urls]\n",
    "    \n",
    "        for i in range(len(sentences_splitted)):\n",
    "            while len(sentences_splitted[i])<150:\n",
    "                sentences_splitted[i].append('<PAD>')\n",
    "        \n",
    "        words_splitted = [[word.split() for word in sentence] for sentence in sentences_splitted]\n",
    "\n",
    "        word_char_sequences = []\n",
    "        \n",
    "        for sentence in words_splitted:\n",
    "            sentence_tokenized = []\n",
    "            for i in range(len(sentence)):\n",
    "                word = self.char_tokenizer.texts_to_sequences(sentence[i])\n",
    "                word_char_padded = pad_sequences(word,maxlen=MAX_WORD_LEN,padding='post')\n",
    "                sentence_tokenized.append(word_char_padded)\n",
    "            word_char_sequences.append(sentence_tokenized)\n",
    "      \n",
    "        word_char_sequences = np.array(word_char_sequences)\n",
    "        word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n",
    "        \n",
    "        return word_char_sequences\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae70f16-47ae-42b8-8bf1-9fdd3e79e614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a627d-491e-47df-8799-696342a19ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f4198-2054-45aa-b67b-c14b081aec08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04296863-009e-4a5a-8a45-3c082050421a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17049961-3113-48d9-b6ac-8e7fe76147bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0dc676-8d3f-4d13-b83d-cf4d3c41cd10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4b06e-f62b-4ad1-bc40-ce18f77c4862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d61d7b-b0b2-402b-be4b-93b46e6db3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "837e796c-0b9f-40c4-a47b-841d60632495",
   "metadata": {},
   "source": [
    "# Character level CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74be879-6950-4b58-8cb7-476c8a46a126",
   "metadata": {},
   "source": [
    "### Character embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61540b00-775d-454d-bc19-0c8d15870246",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_input = Input((MAX_URL_LEN_CHAR,))\n",
    "\n",
    "# Embedding layer\n",
    "char_embedding = Embedding(input_dim=len(char_tokenizer.word_index) + 1, output_dim=k, input_length=MAX_URL_LEN_CHAR)(char_input)\n",
    "char_embedding = tf.expand_dims(char_embedding, -1)  # Add channel dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc5240-1a1f-4970-bc31-daf77f09f632",
   "metadata": {},
   "source": [
    "# CNN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09304aab-616f-4e73-b7c1-94c3afa40c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution and pooling for character-level\n",
    "# h = 3\n",
    "conv_3_char = Conv2D(num_filters, (3, k), activation='relu')(char_embedding)\n",
    "conv_3_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_3_char)\n",
    "# h = 4\n",
    "conv_4_char = Conv2D(num_filters, (4, k), activation='relu')(char_embedding)\n",
    "conv_4_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_4_char)\n",
    "# h = 5\n",
    "conv_5_char = Conv2D(num_filters, (5, k), activation='relu')(char_embedding)\n",
    "conv_5_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_5_char)\n",
    "# h = 6\n",
    "conv_6_char = Conv2D(num_filters, (6, k), activation='relu')(char_embedding)\n",
    "conv_6_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_6_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327cf171-b6ac-469d-96c0-1a0cfc727537",
   "metadata": {},
   "source": [
    "### Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83263576-ddc6-4ee2-9d81-6274fc249abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all convolutional layer outputs\n",
    "concatenated = Concatenate(axis=1)([conv_3_char,conv_4_char,conv_5_char,conv_6_char])\n",
    "flattened = Flatten()(concatenated)\n",
    "\n",
    "# feed concatenated conv layers to fully conected layer\n",
    "dense_char = Dense(512,activation='relu')(flattened)\n",
    "dropout = Dropout(0.5)(dense_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de878ba-24e2-4a1f-971f-707e1217e7ee",
   "metadata": {},
   "source": [
    "# Word-level Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed5e13-8e51-49e9-9fc0-8a6bd18f1733",
   "metadata": {},
   "source": [
    "##### word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee2a87e9-0061-4b28-89d8-f587b1072ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 150, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_input = Input(shape=(MAX_URL_LEN_WORD,))\n",
    "\n",
    "# Embedding layer\n",
    "word_embedding = Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=k, input_length=MAX_URL_LEN_WORD)(word_input)\n",
    "\n",
    "word_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c36b41-0923-4fd9-a689-5647ac5d3035",
   "metadata": {},
   "source": [
    "##### char embedding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f40d8bc1-9d92-4348-ad7f-951306206316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 150, 20, 32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_char_input = Input(shape=(MAX_URL_LEN_CHAR,MAX_WORD_LEN))\n",
    "\n",
    "# Embedding layer\n",
    "word_char_embedding = Embedding(input_dim=len(char_tokenizer.word_index) + 1, output_dim=k, input_length=MAX_URL_LEN_CHAR)(word_char_input)\n",
    "word_char_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c0756-aee1-4a54-a1f6-9f1c653fa517",
   "metadata": {},
   "source": [
    "##### Sum over characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39bcff9a-b91e-4115-aa68-55567725db04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 150, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_layer = tf.keras.layers.Lambda(lambda x: tf.math.reduce_sum(x, axis=2))(word_char_embedding)\n",
    "pooled_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e6593-44e7-457a-8518-d4053b1e95d7",
   "metadata": {},
   "source": [
    "#### Element-wise Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70647042-c576-4d73-a11a-ef44ed6f5396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 150, 32, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition_layer = Add()([pooled_layer, word_embedding])\n",
    "addition_layer = tf.expand_dims(addition_layer,-1)\n",
    "\n",
    "addition_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315cd81-7ce6-4ba7-a08c-720deaf0e572",
   "metadata": {},
   "source": [
    "## Word-level CNN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f273801-80ae-4f8b-a88b-f852554a5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution and pooling for word-level\n",
    "# h = 3\n",
    "conv_3_word = Conv2D(num_filters, (3, k), activation='relu')(addition_layer)\n",
    "conv_3_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_3_word)\n",
    "# h = 4\n",
    "conv_4_word = Conv2D(num_filters, (4, k), activation='relu')(addition_layer)\n",
    "conv_4_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_4_word)\n",
    "# h = 5\n",
    "conv_5_word = Conv2D(num_filters, (5, k), activation='relu')(addition_layer)\n",
    "conv_5_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_5_word)\n",
    "# h = 6\n",
    "conv_6_word = Conv2D(num_filters, (6, k), activation='relu')(addition_layer)\n",
    "conv_6_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_6_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a7e21-a6da-4370-90fe-ea4630a8b8a1",
   "metadata": {},
   "source": [
    "## Word level fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46f6d25b-89f8-480c-b4fd-b7a5caadf213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate all convolutional layer outputs\n",
    "concatenated_word = Concatenate(axis=1)([conv_3_word,conv_4_word,conv_5_word,conv_6_word])\n",
    "flattened_word = Flatten()(concatenated_word)\n",
    "\n",
    "# feed concatenated conv layers to fully conected layer\n",
    "dense_word = Dense(512,activation='relu')(flattened_word)\n",
    "dropout_word = Dropout(0.5)(dense_word)\n",
    "dropout_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40542eaf-16c8-422c-a74f-2404b82f803a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb4e3625-ac35-42be-9bf5-c1b357c64130",
   "metadata": {},
   "source": [
    "## concatenate outputs of char-level and word-level blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "909e1e11-355e-4581-ace8-e352b24d5304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1024])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenate_all = Concatenate()([dropout,dropout_word])\n",
    "concatenate_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0051b07-0774-4c98-8476-e3639aff5abf",
   "metadata": {},
   "source": [
    "# last fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f3a8963-9f6d-4351-8881-52af23d07ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 128])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_1 = Dense(512,activation='relu')(concatenate_all)\n",
    "dropout_fc_1 = Dropout(0.5)(fc_1)\n",
    "\n",
    "fc_2 = Dense(256,activation='relu')(fc_1)\n",
    "dropout_fc_2 = Dropout(0.5)(fc_2)\n",
    "\n",
    "fc_3 = Dense(128,activation='relu')(fc_2)\n",
    "dropout_fc_3 = Dropout(0.5)(fc_3)\n",
    "dropout_fc_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2386c9-6f16-4b23-998a-abcaeecc068e",
   "metadata": {},
   "source": [
    "#### output layer with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "179c97ff-8d75-4bdc-8008-ee1a4244cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Dense(num_classes, activation='softmax')(dropout_fc_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74a465-0c35-4c17-9b50-0044de8810dc",
   "metadata": {},
   "source": [
    "# Model compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce493f05-7754-4bbb-aa90-0b6a4ecb94f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 150, 20)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 150, 20, 32)  2272        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 150)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 150, 32)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 32)      32425216    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 150, 32)      2272        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 150, 32)      0           lambda[0][0]                     \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)     (None, 150, 32, 1)   0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (None, 150, 32, 1)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 148, 1, 256)  24832       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 147, 1, 256)  33024       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 146, 1, 256)  41216       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 145, 1, 256)  49408       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 148, 1, 256)  24832       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 147, 1, 256)  33024       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 146, 1, 256)  41216       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 145, 1, 256)  49408       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 74, 1, 256)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 73, 1, 256)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 73, 1, 256)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 72, 1, 256)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 74, 1, 256)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 73, 1, 256)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 73, 1, 256)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 72, 1, 256)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 292, 1, 256)  0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 292, 1, 256)  0           max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 74752)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 74752)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          38273536    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          38273536    flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1024)         0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          524800      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          131328      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          32896       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 15)           1935        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 109,964,751\n",
      "Trainable params: 109,964,751\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[char_input, word_input, word_char_input],outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bd27d5-b73e-4818-8c6f-55db9540fefa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1805c7a3-c000-47df-b64e-ed24dc0dd92d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
