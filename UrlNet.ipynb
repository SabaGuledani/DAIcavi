{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a38068-d7f9-4899-b84a-c113be987162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "from tensorflow.keras.layers import Input, Dense,Concatenate, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, MaxPooling2D,Dropout, Add,Embedding\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5a2a6d-88b5-41f5-a4a6-96646dd58c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_URL_LEN_CHAR = 110\n",
    "MAX_URL_LEN_WORD = 110\n",
    "MAX_WORD_LEN = 15\n",
    "k = 32\n",
    "num_filters = 256\n",
    "batch_size = 32\n",
    "VAL_FREQ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b8a87e-c42e-4ad0-8682-ce6688cbe99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('casino.db')\n",
    "\n",
    "# df = pd.read_sql('select * from web_pages',conn)\n",
    "# df.drop(columns=[\"id\",\"url\",\"html\",\"screenshot\"],inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76006942-f578-471a-88d1-4662440117cc",
   "metadata": {},
   "source": [
    "# Load dmoz and porn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6490f81-6890-4dd4-a271-e992a9a551a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1854441\n",
      "1822814\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/dmoz.csv')\n",
    "df_porn_casino = pd.read_csv('./porn_casino_links.csv')\n",
    "df_porn_casino = df_porn_casino[:200000]\n",
    "df_malicious = pd.read_csv('malicious_links.csv')\n",
    "df = pd.concat([df,df_malicious,df_porn_casino],ignore_index=True)\n",
    "\n",
    "print(len(df))\n",
    "#drop Nans\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "print(len(df))\n",
    "# shuffle the data\n",
    "df = df.sample(frac=1, random_state=44).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8204051-42c7-47e4-9f8e-419b401d49a8",
   "metadata": {},
   "source": [
    "# Load malicious links dataset and merge with main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d80e70-862a-405f-b04d-b8eb9d9f44d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classes = sorted(list(set(df.category.values)))\n",
    "num_classes = len(classes)\n",
    "print(num_classes)\n",
    "\n",
    "class_to_int = { word:i for i,word in enumerate(classes)}\n",
    "int_to_class = {i:word for i,word in enumerate(classes)}\n",
    "\n",
    "encode = lambda class_: class_to_int[class_]\n",
    "\n",
    "df['category'] = df['category'].apply(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbc03e50-9506-4778-b9f5-723b1e09df58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training: 1458251 for testing: 182281 for validating: 182282\n",
      "http://www.bckayaks.com 2\n"
     ]
    }
   ],
   "source": [
    "train_split = int(0.8 * len(df))\n",
    "x_train = df.url.values[:train_split]\n",
    "y_train = df.category.values[:train_split]\n",
    "\n",
    "x_test = df.url.values[train_split:train_split+ int(0.1 * len(df)) ]\n",
    "y_test =  df.category.values[train_split:train_split+ int(0.1 * len(df))]\n",
    "\n",
    "x_val = df.url.values[train_split+ int(0.1 * len(df)): ]\n",
    "y_val =  df.category.values[train_split+ int(0.1 * len(df)): ]\n",
    "print(\"for training: \" + str(len(x_train))+ \" for testing: \" + str(len(x_test))+ \" for validating: \" + str(len(x_val)))\n",
    "print(x_train[0],y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85721c7-357b-455d-8577-f8a55fdc6a47",
   "metadata": {},
   "source": [
    "# Tokenizers and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3173b-2739-48cf-81bc-819eb9845db7",
   "metadata": {},
   "source": [
    "## Character level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2706f935-edca-4632-9415-fbcf68300ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character tokenization\n",
    "char_tokenizer = Tokenizer(char_level=True, oov_token='<OOV>')\n",
    "char_tokenizer.fit_on_texts(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a4c253-ec28-4baa-af68-8fd7abdd7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_sequence(\n",
    "    input_text,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "):\n",
    "    \n",
    "    if lower:\n",
    "        input_text = input_text.lower()\n",
    "\n",
    "    translate_dict = {c: split for c in filters}\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    input_text = input_text.translate(translate_map)\n",
    "\n",
    "    seq = input_text.split(split)\n",
    "    return [i for i in seq if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab3535-0a29-4902-b327-35b19734fec8",
   "metadata": {},
   "source": [
    "### Word level tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290ba02-e7e2-4bde-bf56-bb469f3510aa",
   "metadata": {},
   "source": [
    "#### Calculate number of words which appear more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e5e0cd6-4971-4a0d-a59a-d149c92327d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_calculation():\n",
    "    word_sequences = [text_to_word_sequence(sentence) for sentence in x_train]\n",
    "    all_words_concat = [item for sublist in word_sequences for item in sublist]\n",
    "    freq_dict_words = Counter(all_words_concat)\n",
    "    \n",
    "    low_freq_words_count = 0\n",
    "    for key in freq_dict_words:\n",
    "        if freq_dict_words[key] == 1:\n",
    "            low_freq_words_count += 1\n",
    "    \n",
    "    high_freq_word_count = int(len(freq_dict_words) * (1-(low_freq_words_count/len(freq_dict_words))))\n",
    "    print(high_freq_word_count)\n",
    "    return high_freq_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6287bae-e6a6-44e0-a536-7dc684628db2",
   "metadata": {},
   "source": [
    "#### Url into sequence of word ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ddcdf90-bbc5-42a9-96fc-2b1d0dd53d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "188661"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_freq_word_count = frequency_calculation()\n",
    "high_freq_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e575df-1a1b-4144-8749-8bdbe5128187",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer(oov_token='<OOV>',num_words=high_freq_word_count)\n",
    "word_tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c86ff-0d34-4179-a329-4e88fc911bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c8fb5ec-667d-45c1-ac9b-2b3546b711b8",
   "metadata": {},
   "source": [
    "## define DataGenerator for preprocessing(tokenizing and padding) inputs on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc050e31-9d4a-42c8-a76b-39ff17fd66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, urls, labels, batch_size, char_tokenizer, word_tokenizer):\n",
    "        self.urls = urls\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.char_tokenizer = char_tokenizer\n",
    "        self.word_tokenizer = word_tokenizer\n",
    "        self.indexes = np.arange(len(urls))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.urls) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_urls = [self.urls[i] for i in batch_indexes]\n",
    "        batch_labels = [self.labels[i] for i in batch_indexes]\n",
    "        \n",
    "        char_input_data = self._preprocess_char_input(batch_urls)\n",
    "        word_input_data = self._preprocess_word_input(batch_urls)\n",
    "        word_char_input_data = self._preprocess_word_char_input(batch_urls)\n",
    "        \n",
    "        return [char_input_data, word_input_data, word_char_input_data], np.array(batch_labels)\n",
    "    \n",
    "    def _preprocess_char_input(self, urls):\n",
    "        char_sequences = self.char_tokenizer.texts_to_sequences(urls)\n",
    "        return pad_sequences(char_sequences, maxlen=MAX_URL_LEN_CHAR, padding='post',truncating='post')\n",
    "    \n",
    "    def _preprocess_word_input(self, urls):\n",
    "        word_sequences = self.word_tokenizer.texts_to_sequences(urls)\n",
    "        return pad_sequences(word_sequences, maxlen=MAX_URL_LEN_WORD, padding='post',truncating='post')\n",
    "    \n",
    "    def _preprocess_word_char_input(self, urls):\n",
    "        sentences_splitted = [text_to_word_sequence(sentence) for sentence in urls]\n",
    "    \n",
    "        for i in range(len(sentences_splitted)):\n",
    "            if len(sentences_splitted[i])>MAX_URL_LEN_CHAR:\n",
    "                sentences_splitted[i] = sentences_splitted[i][:MAX_URL_LEN_CHAR]\n",
    "            else:\n",
    "                while len(sentences_splitted[i])<MAX_URL_LEN_CHAR:\n",
    "                    sentences_splitted[i].append('<OOV>')\n",
    "            \n",
    "        words_splitted = [[word.split() for word in sentence] for sentence in sentences_splitted]\n",
    "\n",
    "        word_char_sequences = []\n",
    "        \n",
    "        for sentence in words_splitted:\n",
    "            sentence_tokenized = []\n",
    "            for i in range(len(sentence)):\n",
    "                word = self.char_tokenizer.texts_to_sequences(sentence[i])\n",
    "                word_char_padded = pad_sequences(word,maxlen=MAX_WORD_LEN,padding='post',truncating='post')\n",
    "                sentence_tokenized.append(word_char_padded)\n",
    "            word_char_sequences.append(sentence_tokenized)\n",
    "        try:\n",
    "            word_char_sequences = np.array(word_char_sequences)\n",
    "        except:\n",
    "            print(self.url)\n",
    "        word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n",
    "        \n",
    "        return word_char_sequences\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dae70f16-47ae-42b8-8bf1-9fdd3e79e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(x_train, y_train, batch_size, char_tokenizer, word_tokenizer)\n",
    "test_generator = DataGenerator(x_test, y_test, batch_size, char_tokenizer, word_tokenizer)\n",
    "val_generator = DataGenerator(x_val, y_val, batch_size, char_tokenizer, word_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e796c-0b9f-40c4-a47b-841d60632495",
   "metadata": {},
   "source": [
    "# Character level CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74be879-6950-4b58-8cb7-476c8a46a126",
   "metadata": {},
   "source": [
    "### Character embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61540b00-775d-454d-bc19-0c8d15870246",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_input = Input((MAX_URL_LEN_CHAR,))\n",
    "\n",
    "# Embedding layer\n",
    "char_embedding = Embedding(input_dim=len(char_tokenizer.word_index) + 1, output_dim=k, input_length=MAX_URL_LEN_CHAR)(char_input)\n",
    "char_embedding = tf.expand_dims(char_embedding, -1)  # Add channel dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc5240-1a1f-4970-bc31-daf77f09f632",
   "metadata": {},
   "source": [
    "# CNN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09304aab-616f-4e73-b7c1-94c3afa40c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution and pooling for character-level\n",
    "# h = 3\n",
    "conv_3_char = Conv2D(num_filters, (3, k), activation='relu')(char_embedding)\n",
    "conv_3_char = BatchNormalization()(conv_3_char)  # Batch normalization after convolution\n",
    "conv_3_char = Activation('relu')(conv_3_char) \n",
    "conv_3_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_3_char)\n",
    "# h = 4\n",
    "conv_4_char = Conv2D(num_filters, (4, k), activation='relu')(char_embedding)\n",
    "conv_4_char = BatchNormalization()(conv_4_char)  # Batch normalization after convolution\n",
    "conv_4_char = Activation('relu')(conv_4_char) \n",
    "conv_4_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_4_char)\n",
    "# h = 5\n",
    "conv_5_char = Conv2D(num_filters, (5, k), activation='relu')(char_embedding)\n",
    "conv_5_char = BatchNormalization()(conv_5_char)  # Batch normalization after convolution\n",
    "conv_5_char = Activation('relu')(conv_5_char) \n",
    "conv_5_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_5_char)\n",
    "# h = 6\n",
    "conv_6_char = Conv2D(num_filters, (6, k), activation='relu')(char_embedding)\n",
    "conv_6_char = BatchNormalization()(conv_6_char)  # Batch normalization after convolution\n",
    "conv_6_char = Activation('relu')(conv_6_char) \n",
    "conv_6_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_6_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327cf171-b6ac-469d-96c0-1a0cfc727537",
   "metadata": {},
   "source": [
    "### Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83263576-ddc6-4ee2-9d81-6274fc249abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all convolutional layer outputs\n",
    "concatenated = Concatenate(axis=1)([conv_3_char,conv_4_char,conv_5_char,conv_6_char])\n",
    "flattened = Flatten()(concatenated)\n",
    "\n",
    "# feed concatenated conv layers to fully conected layer\n",
    "dense_char = Dense(512,activation='relu')(flattened)\n",
    "dropout = Dropout(0.5)(dense_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de878ba-24e2-4a1f-971f-707e1217e7ee",
   "metadata": {},
   "source": [
    "# Word-level Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c666a-004e-4a82-af38-12b26fd99b78",
   "metadata": {},
   "source": [
    "##### reduce the size of word_index to reduce embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e4eb4cf-39dc-4a04-8824-236b699295b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_words = dict(list(word_tokenizer.word_index.items())[:high_freq_word_count])\n",
    "most_freq_words[word_tokenizer.oov_token] = high_freq_word_count + 1  # Add OOV token index manually\n",
    "\n",
    "# Step 3: Use the length of this new word index for the embedding layer\n",
    "input_dim = len(most_freq_words) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed5e13-8e51-49e9-9fc0-8a6bd18f1733",
   "metadata": {},
   "source": [
    "##### word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee2a87e9-0061-4b28-89d8-f587b1072ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 110, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_input = Input(shape=(MAX_URL_LEN_WORD,))\n",
    "\n",
    "# Embedding layer\n",
    "word_embedding = Embedding(input_dim=input_dim, output_dim=k, input_length=MAX_URL_LEN_WORD)(word_input)\n",
    "\n",
    "word_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c36b41-0923-4fd9-a689-5647ac5d3035",
   "metadata": {},
   "source": [
    "##### char embedding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f40d8bc1-9d92-4348-ad7f-951306206316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 110, 15, 32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_char_input = Input(shape=(MAX_URL_LEN_CHAR,MAX_WORD_LEN))\n",
    "\n",
    "# Embedding layer\n",
    "word_char_embedding = Embedding(input_dim=len(char_tokenizer.word_index) + 1, output_dim=k, input_length=MAX_URL_LEN_CHAR)(word_char_input)\n",
    "word_char_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c0756-aee1-4a54-a1f6-9f1c653fa517",
   "metadata": {},
   "source": [
    "##### Sum over characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39bcff9a-b91e-4115-aa68-55567725db04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 110, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_layer = tf.keras.layers.Lambda(lambda x: tf.math.reduce_sum(x, axis=2))(word_char_embedding)\n",
    "pooled_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e6593-44e7-457a-8518-d4053b1e95d7",
   "metadata": {},
   "source": [
    "#### Element-wise Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70647042-c576-4d73-a11a-ef44ed6f5396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 110, 32, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition_layer = Add()([pooled_layer, word_embedding])\n",
    "addition_layer = tf.expand_dims(addition_layer,-1)\n",
    "\n",
    "addition_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315cd81-7ce6-4ba7-a08c-720deaf0e572",
   "metadata": {},
   "source": [
    "## Word-level CNN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f273801-80ae-4f8b-a88b-f852554a5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution and pooling for word-level\n",
    "# h = 3\n",
    "conv_3_word = Conv2D(num_filters, (3, k))(addition_layer)\n",
    "conv_3_word = BatchNormalization()(conv_3_word)  # Batch normalization after convolution\n",
    "conv_3_word = Activation('relu')(conv_3_word) \n",
    "conv_3_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_3_word)\n",
    "# h = 4\n",
    "conv_4_word = Conv2D(num_filters, (4, k), activation='relu')(addition_layer)\n",
    "conv_4_word = BatchNormalization()(conv_4_word)  # Batch normalization after convolution\n",
    "conv_4_word = Activation('relu')(conv_4_word) \n",
    "conv_4_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_4_word)\n",
    "# h = 5\n",
    "conv_5_word = Conv2D(num_filters, (5, k), activation='relu')(addition_layer)\n",
    "conv_5_word = BatchNormalization()(conv_5_word)  # Batch normalization after convolution\n",
    "conv_5_word = Activation('relu')(conv_5_word) \n",
    "conv_5_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_5_word)\n",
    "# h = 6\n",
    "conv_6_word = Conv2D(num_filters, (6, k), activation='relu')(addition_layer)\n",
    "conv_6_word = BatchNormalization()(conv_6_word)  # Batch normalization after convolution\n",
    "conv_6_word = Activation('relu')(conv_6_word) \n",
    "conv_6_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_6_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a7e21-a6da-4370-90fe-ea4630a8b8a1",
   "metadata": {},
   "source": [
    "## Word level fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46f6d25b-89f8-480c-b4fd-b7a5caadf213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate all convolutional layer outputs\n",
    "concatenated_word = Concatenate(axis=1)([conv_3_word,conv_4_word,conv_5_word,conv_6_word])\n",
    "flattened_word = Flatten()(concatenated_word)\n",
    "\n",
    "# feed concatenated conv layers to fully conected layer\n",
    "dense_word = Dense(512,activation='relu')(flattened_word)\n",
    "dropout_word = Dropout(0.5)(dense_word)\n",
    "dropout_word.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e3625-ac35-42be-9bf5-c1b357c64130",
   "metadata": {},
   "source": [
    "## concatenate outputs of char-level and word-level blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "909e1e11-355e-4581-ace8-e352b24d5304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1024])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenate_all = Concatenate()([dropout,dropout_word])\n",
    "concatenate_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0051b07-0774-4c98-8476-e3639aff5abf",
   "metadata": {},
   "source": [
    "# last fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f3a8963-9f6d-4351-8881-52af23d07ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_1 = Dense(512,activation='relu')(concatenate_all)\n",
    "dropout_fc_1 = Dropout(0.5)(fc_1)\n",
    "\n",
    "fc_2 = Dense(256,activation='relu')(dropout_fc_1)\n",
    "dropout_fc_2 = Dropout(0.5)(fc_2)\n",
    "\n",
    "fc_3 = Dense(128,activation='relu')(dropout_fc_2)\n",
    "dropout_fc_3 = Dropout(0.5)(fc_3)\n",
    "\n",
    "# output layer with softmax\n",
    "output = Dense(num_classes, activation='softmax')(dropout_fc_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74a465-0c35-4c17-9b50-0044de8810dc",
   "metadata": {},
   "source": [
    "# Model compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "365d22b8-0c38-4644-89f9-55e6e6fb38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce493f05-7754-4bbb-aa90-0b6a4ecb94f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 110, 15)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 110, 15, 32)  4608        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 110)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 110)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 110, 32)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 110, 32)      6037184     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 110, 32)      4608        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 110, 32)      0           lambda[0][0]                     \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)     (None, 110, 32, 1)   0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (None, 110, 32, 1)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 108, 1, 256)  24832       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 107, 1, 256)  33024       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 106, 1, 256)  41216       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 105, 1, 256)  49408       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 108, 1, 256)  24832       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 107, 1, 256)  33024       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 106, 1, 256)  41216       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 105, 1, 256)  49408       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 108, 1, 256)  1024        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 107, 1, 256)  1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 106, 1, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 105, 1, 256)  1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 108, 1, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 107, 1, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 106, 1, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 105, 1, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 108, 1, 256)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 107, 1, 256)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 106, 1, 256)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 105, 1, 256)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 108, 1, 256)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 107, 1, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 106, 1, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 105, 1, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 54, 1, 256)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 53, 1, 256)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 53, 1, 256)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 52, 1, 256)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 54, 1, 256)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 53, 1, 256)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 53, 1, 256)   0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 52, 1, 256)   0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 212, 1, 256)  0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 212, 1, 256)  0           max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 54272)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 54272)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          27787776    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          27787776    flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1024)         0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          524800      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          131328      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          32896       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           2064        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 62,618,192\n",
      "Trainable params: 62,614,096\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[char_input, word_input, word_char_input],outputs=output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce833b10-8bff-40e0-8ccd-b146fe9f94ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      " 1371/45571 [..............................] - ETA: 30:34 - loss: 2.2171 - accuracy: 0.3212"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    training_generator,\n",
    "    # [char_input_data, word_input_data, word_char_input_data],\n",
    "    # y_train,\n",
    "    validation_data=val_generator,\n",
    "    epochs=14,\n",
    "    validation_freq=4,\n",
    "    callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d49bda-5910-4ec9-ba97-8ec71d9471db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
