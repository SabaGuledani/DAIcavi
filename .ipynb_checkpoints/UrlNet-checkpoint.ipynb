{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a38068-d7f9-4899-b84a-c113be987162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "from tensorflow.keras.layers import Input, Dense,Concatenate, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, MaxPooling2D,Dropout, Add,Embedding\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5a2a6d-88b5-41f5-a4a6-96646dd58c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_URL_LEN_CHAR = 110\n",
    "MAX_URL_LEN_WORD = 110\n",
    "MAX_WORD_LEN = 15\n",
    "k = 32\n",
    "num_filters = 256\n",
    "batch_size = 32\n",
    "VAL_FREQ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b8a87e-c42e-4ad0-8682-ce6688cbe99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('casino.db')\n",
    "\n",
    "# df = pd.read_sql('select * from web_pages',conn)\n",
    "# df.drop(columns=[\"id\",\"url\",\"html\",\"screenshot\"],inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76006942-f578-471a-88d1-4662440117cc",
   "metadata": {},
   "source": [
    "# Load dmoz and porn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6490f81-6890-4dd4-a271-e992a9a551a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1854441\n",
      "1822814\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/dmoz.csv')\n",
    "df_porn_casino = pd.read_csv('./porn_casino_links.csv')\n",
    "df_porn_casino = df_porn_casino[:200000]\n",
    "df_malicious = pd.read_csv('malicious_links.csv')\n",
    "df = pd.concat([df,df_malicious,df_porn_casino],ignore_index=True)\n",
    "\n",
    "print(len(df))\n",
    "#drop Nans\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "print(len(df))\n",
    "# shuffle the data\n",
    "df = df.sample(frac=1, random_state=44).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8204051-42c7-47e4-9f8e-419b401d49a8",
   "metadata": {},
   "source": [
    "# Load malicious links dataset and merge with main df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d80e70-862a-405f-b04d-b8eb9d9f44d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classes = sorted(list(set(df.category.values)))\n",
    "num_classes = len(classes)\n",
    "print(num_classes)\n",
    "\n",
    "class_to_int = { word:i for i,word in enumerate(classes)}\n",
    "int_to_class = {i:word for i,word in enumerate(classes)}\n",
    "\n",
    "encode = lambda class_: class_to_int[class_]\n",
    "\n",
    "df['category'] = df['category'].apply(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbc03e50-9506-4778-b9f5-723b1e09df58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training: 1458251 for testing: 182281 for validating: 182282\n",
      "http://www.bckayaks.com 2\n"
     ]
    }
   ],
   "source": [
    "train_split = int(0.8 * len(df))\n",
    "x_train = df.url.values[:train_split]\n",
    "y_train = df.category.values[:train_split]\n",
    "\n",
    "x_test = df.url.values[train_split:train_split+ int(0.1 * len(df)) ]\n",
    "y_test =  df.category.values[train_split:train_split+ int(0.1 * len(df))]\n",
    "\n",
    "x_val = df.url.values[train_split+ int(0.1 * len(df)): ]\n",
    "y_val =  df.category.values[train_split+ int(0.1 * len(df)): ]\n",
    "print(\"for training: \" + str(len(x_train))+ \" for testing: \" + str(len(x_test))+ \" for validating: \" + str(len(x_val)))\n",
    "print(x_train[0],y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85721c7-357b-455d-8577-f8a55fdc6a47",
   "metadata": {},
   "source": [
    "# Tokenizers and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3173b-2739-48cf-81bc-819eb9845db7",
   "metadata": {},
   "source": [
    "## Character level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2706f935-edca-4632-9415-fbcf68300ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character tokenization\n",
    "char_tokenizer = Tokenizer(char_level=True, oov_token='<OOV>')\n",
    "char_tokenizer.fit_on_texts(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67a4c253-ec28-4baa-af68-8fd7abdd7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_sequence(\n",
    "    input_text,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "):\n",
    "    \n",
    "    if lower:\n",
    "        input_text = input_text.lower()\n",
    "\n",
    "    translate_dict = {c: split for c in filters}\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    input_text = input_text.translate(translate_map)\n",
    "\n",
    "    seq = input_text.split(split)\n",
    "    return [i for i in seq if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab3535-0a29-4902-b327-35b19734fec8",
   "metadata": {},
   "source": [
    "### Word level tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290ba02-e7e2-4bde-bf56-bb469f3510aa",
   "metadata": {},
   "source": [
    "#### Calculate number of words which appear more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e5e0cd6-4971-4a0d-a59a-d149c92327d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_calculation():\n",
    "    word_sequences = [text_to_word_sequence(sentence) for sentence in x_train]\n",
    "    all_words_concat = [item for sublist in word_sequences for item in sublist]\n",
    "    freq_dict_words = Counter(all_words_concat)\n",
    "    \n",
    "    low_freq_words_count = 0\n",
    "    for key in freq_dict_words:\n",
    "        if freq_dict_words[key] == 1:\n",
    "            low_freq_words_count += 1\n",
    "    \n",
    "    high_freq_word_count = int(len(freq_dict_words) * (1-(low_freq_words_count/len(freq_dict_words))))\n",
    "    print(high_freq_word_count)\n",
    "    return high_freq_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6287bae-e6a6-44e0-a536-7dc684628db2",
   "metadata": {},
   "source": [
    "#### Url into sequence of word ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ddcdf90-bbc5-42a9-96fc-2b1d0dd53d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188661\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "188661"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_freq_word_count = frequency_calculation()\n",
    "high_freq_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7e575df-1a1b-4144-8749-8bdbe5128187",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer(oov_token='<OOV>',num_words=high_freq_word_count)\n",
    "word_tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d52c86ff-0d34-4179-a329-4e88fc911bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['http://www.newadvent.org/cathen/03079a.htm', 'http://dominasexe.com/', 'http://eclipse.cps.k12.va.us/pta/index.htm', 'http://sexycamgirls.startspot.nl/', 'http://www.pharmacymuseum.org/', 'http://www.sparklesgiftbaskets.com', 'http://sk41wetzlar.de/', 'http://www.gamestats.com/objects/014/014699/', 'http://www.ournightsky.com', 'http://www.fiestaturbo.com', 'http://www.lazaruslives.com/', 'http://www.gospellakes.org/', 'http://stan-dard-chatt-er-ed.tk/Sign_On.php', 'http://www.lysator.liu.se/religion/neopagan/celtic.html', 'http://samanna.net/eq.general/maps.shtml', 'http://sociolingo.wordpress.com/', 'http://www.haro-online.com/movies/notting_hill.html', 'https://637341.8b.io/\\xa0', 'http://collections2.eeb.uconn.edu/collections/cicadacentral/resources/reprints/cooley&amp;marshall_2001.pdf', 'http://www.vh1.com/artists/az/id_1103/artist.jhtml', 'http://www.coloradotaxidermyschool.com/', 'http://www.bettyscarpino.com/', 'http://www.utcoverseas.com/', 'http://www.treasurelore.com/florida/table.htm', 'http://www.internationalscrew.com/', 'http://sexybw.de/', 'http://hansoncity.freeservers.com', 'http://www.glass-tech.com/', 'http://sexcontacten.linkoverzicht.be/', 'http://www.2020eyesite.com/', 'http://free64all.com/', 'http://www.access-music.de/']\n",
    "sentences_splitted = [text_to_word_sequence(sentence) for sentence in urls]\n",
    "for i in range(len(sentences_splitted)):\n",
    "    if len(sentences_splitted[i])>MAX_URL_LEN_CHAR:\n",
    "        sentences_splitted[i] = sentences_splitted[i][:MAX_URL_LEN_CHAR]\n",
    "    else:\n",
    "        while len(sentences_splitted[i])<MAX_URL_LEN_CHAR:\n",
    "            sentences_splitted[i].append('<OOV>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a51506-34b0-4422-ae26-b2467cf14e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9030c652-a238-4321-b491-a30633e63deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.newadvent.org/cathen/03079a.htm', 'http://dominasexe.com/', 'http://eclipse.cps.k12.va.us/pta/index.htm', 'http://sexycamgirls.startspot.nl/', 'http://www.pharmacymuseum.org/', 'http://www.sparklesgiftbaskets.com', 'http://sk41wetzlar.de/', 'http://www.gamestats.com/objects/014/014699/', 'http://www.ournightsky.com', 'http://www.fiestaturbo.com', 'http://www.lazaruslives.com/', 'http://www.gospellakes.org/', 'http://stan-dard-chatt-er-ed.tk/Sign_On.php', 'http://www.lysator.liu.se/religion/neopagan/celtic.html', 'http://samanna.net/eq.general/maps.shtml', 'http://sociolingo.wordpress.com/', 'http://www.haro-online.com/movies/notting_hill.html', 'https://637341.8b.io/\\xa0', 'http://collections2.eeb.uconn.edu/collections/cicadacentral/resources/reprints/cooley&amp;marshall_2001.pdf', 'http://www.vh1.com/artists/az/id_1103/artist.jhtml', 'http://www.coloradotaxidermyschool.com/', 'http://www.bettyscarpino.com/', 'http://www.utcoverseas.com/', 'http://www.treasurelore.com/florida/table.htm', 'http://www.internationalscrew.com/', 'http://sexybw.de/', 'http://hansoncity.freeservers.com', 'http://www.glass-tech.com/', 'http://sexcontacten.linkoverzicht.be/', 'http://www.2020eyesite.com/', 'http://free64all.com/', 'http://www.access-music.de/']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\warfa\\AppData\\Local\\Temp\\ipykernel_20676\\1034088537.py:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  word_char_sequences = np.array(word_char_sequences)\n"
     ]
    }
   ],
   "source": [
    "words_splitted = [[word.split() for word in sentence] for sentence in sentences_splitted]\n",
    "\n",
    "word_char_sequences = []\n",
    "\n",
    "for sentence in words_splitted:\n",
    "    sentence_tokenized = []\n",
    "    for i in range(len(sentence)):\n",
    "        word = char_tokenizer.texts_to_sequences(sentence[i])\n",
    "        word_char_padded = pad_sequences(word,maxlen=MAX_WORD_LEN,padding='post',truncating='post')\n",
    "        sentence_tokenized.append(word_char_padded)\n",
    "    word_char_sequences.append(sentence_tokenized)\n",
    "try:\n",
    "    word_char_sequences = np.array(word_char_sequences)\n",
    "except:\n",
    "    print(urls)\n",
    "try:\n",
    "    word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n",
    "except:\n",
    "    print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "37c7c181-4047-4412-88a2-44ebd2c20b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95837f97-fdaa-4bd5-b34d-646952dde272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 110)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_char_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07e6b524-b8f9-4fe5-92d4-a373ae27e578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_char_sequences.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8fb5ec-667d-45c1-ac9b-2b3546b711b8",
   "metadata": {},
   "source": [
    "## define DataGenerator for preprocessing(tokenizing and padding) inputs on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc050e31-9d4a-42c8-a76b-39ff17fd66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, urls, labels, batch_size, char_tokenizer, word_tokenizer):\n",
    "        self.urls = urls\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.char_tokenizer = char_tokenizer\n",
    "        self.word_tokenizer = word_tokenizer\n",
    "        self.indexes = np.arange(len(urls))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.urls) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_urls = [self.urls[i] for i in batch_indexes]\n",
    "        batch_labels = [self.labels[i] for i in batch_indexes]\n",
    "        \n",
    "        char_input_data = self._preprocess_char_input(batch_urls)\n",
    "        word_input_data = self._preprocess_word_input(batch_urls)\n",
    "        word_char_input_data = self._preprocess_word_char_input(batch_urls)\n",
    "        \n",
    "        return [char_input_data, word_input_data, word_char_input_data], np.array(batch_labels)\n",
    "    \n",
    "    def _preprocess_char_input(self, urls):\n",
    "        char_sequences = self.char_tokenizer.texts_to_sequences(urls)\n",
    "        return pad_sequences(char_sequences, maxlen=MAX_URL_LEN_CHAR, padding='post',truncating='post')\n",
    "    \n",
    "    def _preprocess_word_input(self, urls):\n",
    "        word_sequences = self.word_tokenizer.texts_to_sequences(urls)\n",
    "        return pad_sequences(word_sequences, maxlen=MAX_URL_LEN_WORD, padding='post',truncating='post')\n",
    "    \n",
    "    def _preprocess_word_char_input(self, urls):\n",
    "        sentences_splitted = [text_to_word_sequence(sentence) for sentence in urls]\n",
    "    \n",
    "        for i in range(len(sentences_splitted)):\n",
    "            if len(sentences_splitted[i])>MAX_URL_LEN_CHAR:\n",
    "                sentences_splitted[i] = sentences_splitted[i][:MAX_URL_LEN_CHAR]\n",
    "            else:\n",
    "                while len(sentences_splitted[i])<MAX_URL_LEN_CHAR:\n",
    "                    sentences_splitted[i].append('<OOV>')\n",
    "            \n",
    "        words_splitted = [[word.split() for word in sentence] for sentence in sentences_splitted]\n",
    "\n",
    "        word_char_sequences = []\n",
    "        \n",
    "        for sentence in words_splitted:\n",
    "            sentence_tokenized = []\n",
    "            for i in range(len(sentence)):\n",
    "                word = self.char_tokenizer.texts_to_sequences(sentence[i])\n",
    "                word_char_padded = pad_sequences(word,maxlen=MAX_WORD_LEN,padding='post',truncating='post')\n",
    "                sentence_tokenized.append(word_char_padded)\n",
    "            word_char_sequences.append(sentence_tokenized)\n",
    "        try:\n",
    "            word_char_sequences = np.array(word_char_sequences)\n",
    "        except:\n",
    "            print(urls)\n",
    "        try:\n",
    "            word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n",
    "        except:\n",
    "            print(urls)\n",
    "        return word_char_sequences\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dae70f16-47ae-42b8-8bf1-9fdd3e79e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(x_train, y_train, batch_size, char_tokenizer, word_tokenizer)\n",
    "test_generator = DataGenerator(x_test, y_test, batch_size, char_tokenizer, word_tokenizer)\n",
    "val_generator = DataGenerator(x_val, y_val, batch_size, char_tokenizer, word_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e796c-0b9f-40c4-a47b-841d60632495",
   "metadata": {},
   "source": [
    "# Character level CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74be879-6950-4b58-8cb7-476c8a46a126",
   "metadata": {},
   "source": [
    "### Character embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61540b00-775d-454d-bc19-0c8d15870246",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_input = Input((MAX_URL_LEN_CHAR,))\n",
    "\n",
    "# Embedding layer\n",
    "char_embedding = Embedding(input_dim=len(char_tokenizer.word_index) + 1, output_dim=k, input_length=MAX_URL_LEN_CHAR)(char_input)\n",
    "char_embedding = tf.expand_dims(char_embedding, -1)  # Add channel dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc5240-1a1f-4970-bc31-daf77f09f632",
   "metadata": {},
   "source": [
    "# CNN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09304aab-616f-4e73-b7c1-94c3afa40c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution and pooling for character-level\n",
    "# h = 3\n",
    "conv_3_char = Conv2D(num_filters, (3, k), activation='relu')(char_embedding)\n",
    "conv_3_char = BatchNormalization()(conv_3_char)  # Batch normalization after convolution\n",
    "conv_3_char = Activation('relu')(conv_3_char) \n",
    "conv_3_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_3_char)\n",
    "# h = 4\n",
    "conv_4_char = Conv2D(num_filters, (4, k), activation='relu')(char_embedding)\n",
    "conv_4_char = BatchNormalization()(conv_4_char)  # Batch normalization after convolution\n",
    "conv_4_char = Activation('relu')(conv_4_char) \n",
    "conv_4_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_4_char)\n",
    "# h = 5\n",
    "conv_5_char = Conv2D(num_filters, (5, k), activation='relu')(char_embedding)\n",
    "conv_5_char = BatchNormalization()(conv_5_char)  # Batch normalization after convolution\n",
    "conv_5_char = Activation('relu')(conv_5_char) \n",
    "conv_5_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_5_char)\n",
    "# h = 6\n",
    "conv_6_char = Conv2D(num_filters, (6, k), activation='relu')(char_embedding)\n",
    "conv_6_char = BatchNormalization()(conv_6_char)  # Batch normalization after convolution\n",
    "conv_6_char = Activation('relu')(conv_6_char) \n",
    "conv_6_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_6_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327cf171-b6ac-469d-96c0-1a0cfc727537",
   "metadata": {},
   "source": [
    "### Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83263576-ddc6-4ee2-9d81-6274fc249abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all convolutional layer outputs\n",
    "concatenated = Concatenate(axis=1)([conv_3_char,conv_4_char,conv_5_char,conv_6_char])\n",
    "flattened = Flatten()(concatenated)\n",
    "\n",
    "# feed concatenated conv layers to fully conected layer\n",
    "dense_char = Dense(512,activation='relu')(flattened)\n",
    "dropout = Dropout(0.5)(dense_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de878ba-24e2-4a1f-971f-707e1217e7ee",
   "metadata": {},
   "source": [
    "# Word-level Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c666a-004e-4a82-af38-12b26fd99b78",
   "metadata": {},
   "source": [
    "##### reduce the size of word_index to reduce embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e4eb4cf-39dc-4a04-8824-236b699295b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_words = dict(list(word_tokenizer.word_index.items())[:high_freq_word_count])\n",
    "most_freq_words[word_tokenizer.oov_token] = high_freq_word_count + 1  # Add OOV token index manually\n",
    "\n",
    "# Step 3: Use the length of this new word index for the embedding layer\n",
    "input_dim = len(most_freq_words) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed5e13-8e51-49e9-9fc0-8a6bd18f1733",
   "metadata": {},
   "source": [
    "##### word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee2a87e9-0061-4b28-89d8-f587b1072ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 110, 32])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_input = Input(shape=(MAX_URL_LEN_WORD,))\n",
    "\n",
    "# Embedding layer\n",
    "word_embedding = Embedding(input_dim=input_dim, output_dim=k, input_length=MAX_URL_LEN_WORD)(word_input)\n",
    "\n",
    "word_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c36b41-0923-4fd9-a689-5647ac5d3035",
   "metadata": {},
   "source": [
    "##### char embedding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f40d8bc1-9d92-4348-ad7f-951306206316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 110, 15, 32])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_char_input = Input(shape=(MAX_URL_LEN_CHAR,MAX_WORD_LEN))\n",
    "\n",
    "# Embedding layer\n",
    "word_char_embedding = Embedding(input_dim=len(char_tokenizer.word_index) + 1, output_dim=k, input_length=MAX_URL_LEN_CHAR)(word_char_input)\n",
    "word_char_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c0756-aee1-4a54-a1f6-9f1c653fa517",
   "metadata": {},
   "source": [
    "##### Sum over characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39bcff9a-b91e-4115-aa68-55567725db04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 110, 32])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_layer = tf.keras.layers.Lambda(lambda x: tf.math.reduce_sum(x, axis=2))(word_char_embedding)\n",
    "pooled_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e6593-44e7-457a-8518-d4053b1e95d7",
   "metadata": {},
   "source": [
    "#### Element-wise Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70647042-c576-4d73-a11a-ef44ed6f5396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 110, 32, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition_layer = Add()([pooled_layer, word_embedding])\n",
    "addition_layer = tf.expand_dims(addition_layer,-1)\n",
    "\n",
    "addition_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315cd81-7ce6-4ba7-a08c-720deaf0e572",
   "metadata": {},
   "source": [
    "## Word-level CNN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f273801-80ae-4f8b-a88b-f852554a5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution and pooling for word-level\n",
    "# h = 3\n",
    "conv_3_word = Conv2D(num_filters, (3, k))(addition_layer)\n",
    "conv_3_word = BatchNormalization()(conv_3_word)  # Batch normalization after convolution\n",
    "conv_3_word = Activation('relu')(conv_3_word) \n",
    "conv_3_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_3_word)\n",
    "# h = 4\n",
    "conv_4_word = Conv2D(num_filters, (4, k), activation='relu')(addition_layer)\n",
    "conv_4_word = BatchNormalization()(conv_4_word)  # Batch normalization after convolution\n",
    "conv_4_word = Activation('relu')(conv_4_word) \n",
    "conv_4_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_4_word)\n",
    "# h = 5\n",
    "conv_5_word = Conv2D(num_filters, (5, k), activation='relu')(addition_layer)\n",
    "conv_5_word = BatchNormalization()(conv_5_word)  # Batch normalization after convolution\n",
    "conv_5_word = Activation('relu')(conv_5_word) \n",
    "conv_5_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_5_word)\n",
    "# h = 6\n",
    "conv_6_word = Conv2D(num_filters, (6, k), activation='relu')(addition_layer)\n",
    "conv_6_word = BatchNormalization()(conv_6_word)  # Batch normalization after convolution\n",
    "conv_6_word = Activation('relu')(conv_6_word) \n",
    "conv_6_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_6_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a7e21-a6da-4370-90fe-ea4630a8b8a1",
   "metadata": {},
   "source": [
    "## Word level fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46f6d25b-89f8-480c-b4fd-b7a5caadf213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate all convolutional layer outputs\n",
    "concatenated_word = Concatenate(axis=1)([conv_3_word,conv_4_word,conv_5_word,conv_6_word])\n",
    "flattened_word = Flatten()(concatenated_word)\n",
    "\n",
    "# feed concatenated conv layers to fully conected layer\n",
    "dense_word = Dense(512,activation='relu')(flattened_word)\n",
    "dropout_word = Dropout(0.5)(dense_word)\n",
    "dropout_word.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4e3625-ac35-42be-9bf5-c1b357c64130",
   "metadata": {},
   "source": [
    "## concatenate outputs of char-level and word-level blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "909e1e11-355e-4581-ace8-e352b24d5304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1024])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenate_all = Concatenate()([dropout,dropout_word])\n",
    "concatenate_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0051b07-0774-4c98-8476-e3639aff5abf",
   "metadata": {},
   "source": [
    "# last fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f3a8963-9f6d-4351-8881-52af23d07ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_1 = Dense(512,activation='relu')(concatenate_all)\n",
    "dropout_fc_1 = Dropout(0.5)(fc_1)\n",
    "\n",
    "fc_2 = Dense(256,activation='relu')(dropout_fc_1)\n",
    "dropout_fc_2 = Dropout(0.5)(fc_2)\n",
    "\n",
    "fc_3 = Dense(128,activation='relu')(dropout_fc_2)\n",
    "dropout_fc_3 = Dropout(0.5)(fc_3)\n",
    "\n",
    "# output layer with softmax\n",
    "output = Dense(num_classes, activation='softmax')(dropout_fc_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74a465-0c35-4c17-9b50-0044de8810dc",
   "metadata": {},
   "source": [
    "# Model compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "365d22b8-0c38-4644-89f9-55e6e6fb38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce493f05-7754-4bbb-aa90-0b6a4ecb94f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 110, 15)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 110, 15, 32)  4608        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 110)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 110)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 110, 32)      0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 110, 32)      6037184     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 110, 32)      4608        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 110, 32)      0           lambda[0][0]                     \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)     (None, 110, 32, 1)   0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (None, 110, 32, 1)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 108, 1, 256)  24832       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 107, 1, 256)  33024       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 106, 1, 256)  41216       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 105, 1, 256)  49408       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 108, 1, 256)  24832       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 107, 1, 256)  33024       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 106, 1, 256)  41216       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 105, 1, 256)  49408       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 108, 1, 256)  1024        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 107, 1, 256)  1024        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 106, 1, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 105, 1, 256)  1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 108, 1, 256)  1024        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 107, 1, 256)  1024        conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 106, 1, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 105, 1, 256)  1024        conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 108, 1, 256)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 107, 1, 256)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 106, 1, 256)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 105, 1, 256)  0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 108, 1, 256)  0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 107, 1, 256)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 106, 1, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 105, 1, 256)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 54, 1, 256)   0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 53, 1, 256)   0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 53, 1, 256)   0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 52, 1, 256)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 54, 1, 256)   0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 53, 1, 256)   0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 53, 1, 256)   0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 52, 1, 256)   0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 212, 1, 256)  0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 212, 1, 256)  0           max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 54272)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 54272)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          27787776    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          27787776    flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1024)         0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          524800      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          131328      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          32896       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 16)           2064        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 62,618,192\n",
      "Trainable params: 62,614,096\n",
      "Non-trainable params: 4,096\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[char_input, word_input, word_char_input],outputs=output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce833b10-8bff-40e0-8ccd-b146fe9f94ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      " 1965/45571 [>.............................] - ETA: 31:32 - loss: 2.1523 - accuracy: 0.3487['http://www.newadvent.org/cathen/03079a.htm', 'http://dominasexe.com/', 'http://eclipse.cps.k12.va.us/pta/index.htm', 'http://sexycamgirls.startspot.nl/', 'http://www.pharmacymuseum.org/', 'http://www.sparklesgiftbaskets.com', 'http://sk41wetzlar.de/', 'http://www.gamestats.com/objects/014/014699/', 'http://www.ournightsky.com', 'http://www.fiestaturbo.com', 'http://www.lazaruslives.com/', 'http://www.gospellakes.org/', 'http://stan-dard-chatt-er-ed.tk/Sign_On.php', 'http://www.lysator.liu.se/religion/neopagan/celtic.html', 'http://samanna.net/eq.general/maps.shtml', 'http://sociolingo.wordpress.com/', 'http://www.haro-online.com/movies/notting_hill.html', 'https://637341.8b.io/\\xa0', 'http://collections2.eeb.uconn.edu/collections/cicadacentral/resources/reprints/cooley&amp;marshall_2001.pdf', 'http://www.vh1.com/artists/az/id_1103/artist.jhtml', 'http://www.coloradotaxidermyschool.com/', 'http://www.bettyscarpino.com/', 'http://www.utcoverseas.com/', 'http://www.treasurelore.com/florida/table.htm', 'http://www.internationalscrew.com/', 'http://sexybw.de/', 'http://hansoncity.freeservers.com', 'http://www.glass-tech.com/', 'http://sexcontacten.linkoverzicht.be/', 'http://www.2020eyesite.com/', 'http://free64all.com/', 'http://www.access-music.de/']\n",
      " 1967/45571 [>.............................] - ETA: 31:32 - loss: 2.1519 - accuracy: 0.3488"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\warfa\\AppData\\Local\\Temp\\ipykernel_20676\\3104291058.py:54: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  word_char_sequences = np.array(word_char_sequences)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was [[array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[16,  6,  4, 10, 19, 26,  6, 16,  3,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[19,  7, 13, 14, 16, 10, 12,  6, 29,  6,  0,  0,  0,  0,  0]])\n  array([[11,  7, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[ 6, 11, 17, 14,  9, 12,  6,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[11,  9, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n ...\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[31, 28, 31, 28,  6, 24,  6, 12, 14,  3,  6,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[23, 15,  6,  6, 39, 34, 10, 17, 17,  0,  0,  0,  0,  0,  0]])\n  array([[11,  7, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[10, 11, 11,  6, 12, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]].\nTypeError: only size-1 arrays can be converted to Python scalars\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 911, in generator_py_func\n    script_ops.FuncRegistry._convert(  # pylint: disable=protected-access\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 209, in _convert\n    result = np.asarray(value, dtype=dtype, order=\"C\")\n\nValueError: setting an array element with a sequence.\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 915, in generator_py_func\n    six.reraise(\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\six.py\", line 718, in reraise\n    raise value.with_traceback(tb)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 911, in generator_py_func\n    script_ops.FuncRegistry._convert(  # pylint: disable=protected-access\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 209, in _convert\n    result = np.asarray(value, dtype=dtype, order=\"C\")\n\nTypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was [[array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[16,  6,  4, 10, 19, 26,  6, 16,  3,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[19,  7, 13, 14, 16, 10, 12,  6, 29,  6,  0,  0,  0,  0,  0]])\n  array([[11,  7, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[ 6, 11, 17, 14,  9, 12,  6,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[11,  9, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n ...\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[31, 28, 31, 28,  6, 24,  6, 12, 14,  3,  6,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[23, 15,  6,  6, 39, 34, 10, 17, 17,  0,  0,  0,  0,  0,  0]])\n  array([[11,  7, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[10, 11, 11,  6, 12, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]].\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n  (1) Invalid argument:  TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was [[array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[16,  6,  4, 10, 19, 26,  6, 16,  3,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[19,  7, 13, 14, 16, 10, 12,  6, 29,  6,  0,  0,  0,  0,  0]])\n  array([[11,  7, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0,  [Op:__inference_train_function_3255]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# [char_input_data, word_input_data, word_char_input_data],\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# y_train,\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was [[array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[16,  6,  4, 10, 19, 26,  6, 16,  3,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[19,  7, 13, 14, 16, 10, 12,  6, 29,  6,  0,  0,  0,  0,  0]])\n  array([[11,  7, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[ 6, 11, 17, 14,  9, 12,  6,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[11,  9, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n ...\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[31, 28, 31, 28,  6, 24,  6, 12, 14,  3,  6,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[23, 15,  6,  6, 39, 34, 10, 17, 17,  0,  0,  0,  0,  0,  0]])\n  array([[11,  7, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[10, 11, 11,  6, 12, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]].\nTypeError: only size-1 arrays can be converted to Python scalars\n\n\nThe above exception was the direct cause of the following exception:\n\n\nTraceback (most recent call last):\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 911, in generator_py_func\n    script_ops.FuncRegistry._convert(  # pylint: disable=protected-access\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 209, in _convert\n    result = np.asarray(value, dtype=dtype, order=\"C\")\n\nValueError: setting an array element with a sequence.\n\n\nDuring handling of the above exception, another exception occurred:\n\n\nTraceback (most recent call last):\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 915, in generator_py_func\n    six.reraise(\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\six.py\", line 718, in reraise\n    raise value.with_traceback(tb)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 911, in generator_py_func\n    script_ops.FuncRegistry._convert(  # pylint: disable=protected-access\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 209, in _convert\n    result = np.asarray(value, dtype=dtype, order=\"C\")\n\nTypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was [[array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[16,  6,  4, 10, 19, 26,  6, 16,  3,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[19,  7, 13, 14, 16, 10, 12,  6, 29,  6,  0,  0,  0,  0,  0]])\n  array([[11,  7, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[ 6, 11, 17, 14,  9, 12,  6,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[11,  9, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n ...\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[31, 28, 31, 28,  6, 24,  6, 12, 14,  3,  6,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[23, 15,  6,  6, 39, 34, 10, 17, 17,  0,  0,  0,  0,  0,  0]])\n  array([[11,  7, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[10, 11, 11,  6, 12, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]].\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n  (1) Invalid argument:  TypeError: `generator` yielded an element that could not be converted to the expected type. The expected type was int32, but the yielded element was [[array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[16,  6,  4, 10, 19, 26,  6, 16,  3,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n  array([[19,  7, 13, 14, 16, 10, 12,  6, 29,  6,  0,  0,  0,  0,  0]])\n  array([[11,  7, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  ...\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n  array([[ 1,  7,  7, 26,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])]\n [array([[8, 3, 3, 9, 0, 0,  [Op:__inference_train_function_3255]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    training_generator,\n",
    "    # [char_input_data, word_input_data, word_char_input_data],\n",
    "    # y_train,\n",
    "    validation_data=val_generator,\n",
    "    epochs=14,\n",
    "    validation_freq=4,\n",
    "    callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00d49bda-5910-4ec9-ba97-8ec71d9471db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['http://www.bckayaks.com',\n",
       "       'http://www.mcps.k12.md.us/departments/deafservices/',\n",
       "       'http://www.bfcu.net/', ..., 'http://www.braided-web.net/',\n",
       "       'http://www.kidsreads.com/authors/au-stine-rl.asp',\n",
       "       'http://www.newadvent.org/cathen/03137a.htm'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3bc2a9-cb9c-48df-8d62-44c13ff885be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
