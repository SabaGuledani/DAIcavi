{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a38068-d7f9-4899-b84a-c113be987162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score,confusion_matrix,ConfusionMatrixDisplay\n",
    "from tensorflow.keras.layers import Input, Dense,Concatenate, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, MaxPooling2D,Dropout, Add,Embedding\n",
    "from keras.models import Model, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a5a2a6d-88b5-41f5-a4a6-96646dd58c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_URL_LEN_CHAR = 70\n",
    "MAX_URL_LEN_WORD = 70\n",
    "MAX_WORD_LEN = 15\n",
    "k = 16\n",
    "num_filters = 256\n",
    "batch_size = 32\n",
    "VAL_FREQ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b8a87e-c42e-4ad0-8682-ce6688cbe99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.connect('casino.db')\n",
    "\n",
    "# df = pd.read_sql('select * from web_pages',conn)\n",
    "# df.drop(columns=[\"id\",\"url\",\"html\",\"screenshot\"],inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76006942-f578-471a-88d1-4662440117cc",
   "metadata": {},
   "source": [
    "# Load dmoz dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946f551b-3a3e-49dc-b8de-310b95cf04cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562977\n",
      "1562974\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./datasets/dmoz.csv')\n",
    "print(len(df))\n",
    "#drop Nans\n",
    "df = df.dropna()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b115e9-7f28-433e-b02f-c98a44d89081",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['url'].apply(lambda x: len(x) > 70)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec531aa3-052e-4ed2-9022-3c35186298f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>http://www.hotsexsites.org/free-anime-hentai-m...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>http://www.hotsexsites.org/free-anime-hentai-m...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>http://www.hotsexsites.org/free-anime-hentai-m...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>http://www.hotsexsites.org/free-anime-hentai-m...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>http://www.hotsexsites.org/free-anime-hentai-m...</td>\n",
       "      <td>Adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561981</th>\n",
       "      <td>http://www.goviks.com/sportselect.dbml?temp_si...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561984</th>\n",
       "      <td>http://www.cmuchippewas.com/sportselect.dbml?d...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562057</th>\n",
       "      <td>http://www.negaunee.k12.mi.us/hs_web/athletics...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562649</th>\n",
       "      <td>http://sports.dir.groups.yahoo.com/dir/recreat...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562681</th>\n",
       "      <td>http://www.cmt.com/shows/series/hulk-hogan-cel...</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39793 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       url category\n",
       "143      http://www.hotsexsites.org/free-anime-hentai-m...    Adult\n",
       "146      http://www.hotsexsites.org/free-anime-hentai-m...    Adult\n",
       "149      http://www.hotsexsites.org/free-anime-hentai-m...    Adult\n",
       "151      http://www.hotsexsites.org/free-anime-hentai-m...    Adult\n",
       "152      http://www.hotsexsites.org/free-anime-hentai-m...    Adult\n",
       "...                                                    ...      ...\n",
       "1561981  http://www.goviks.com/sportselect.dbml?temp_si...   Sports\n",
       "1561984  http://www.cmuchippewas.com/sportselect.dbml?d...   Sports\n",
       "1562057  http://www.negaunee.k12.mi.us/hs_web/athletics...   Sports\n",
       "1562649  http://sports.dir.groups.yahoo.com/dir/recreat...   Sports\n",
       "1562681  http://www.cmt.com/shows/series/hulk-hogan-cel...   Sports\n",
       "\n",
       "[39793 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b32bf1-80e4-466e-b48a-e86b9e2b1200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781487\n"
     ]
    }
   ],
   "source": [
    "# shuffle the data\n",
    "df = df.sample(frac=1, random_state=44).reset_index(drop=True)\n",
    "half_df = int((len(df)*0.5))\n",
    "df = df[:half_df]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d463cdd-180c-4054-b687-edbba76e146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(list(set(df.category.values)))\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36029394-5c39-4619-baa8-4ab3062f6406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35160237-4a48-47a5-963d-3b74caea6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_int = { word:i for i,word in enumerate(classes)}\n",
    "int_to_class = {i:word for i,word in enumerate(classes)}\n",
    "\n",
    "encode = lambda class_: class_to_int[class_]\n",
    "\n",
    "df['category'] = df['category'].apply(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbc03e50-9506-4778-b9f5-723b1e09df58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for training: 625189 for testing: 156298\n",
      "http://www.newadvent.org/cathen/11535a.htm 13\n"
     ]
    }
   ],
   "source": [
    "train_split = int(0.8 * len(df))\n",
    "x_train = df.url.values[:train_split]\n",
    "y_train = df.category.values[:train_split]\n",
    "\n",
    "x_test = df.url.values[train_split: ]\n",
    "y_test =  df.category.values[train_split:]\n",
    "\n",
    "print(\"for training: \" + str(len(x_train))+ \" for testing: \" + str(len(x_test)))\n",
    "print(x_train[0],y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "543dc52e-dae1-4a3b-9ba9-b508257d2b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>http://www.preplogic.com/products/exams/practi...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>http://rogerebert.suntimes.com/apps/pbcs.dll/a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>http://www.women.com/entertain/celeb/articles/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>http://www.admin.cam.ac.uk/univ/gsprospectus/c...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>http://www.uwbadgers.com/sport_news/mxc/headli...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781380</th>\n",
       "      <td>http://web.clas.ufl.edu/users/rhatch/pages/02-...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781382</th>\n",
       "      <td>http://www.free-porn-pics-free-porn-pics-free-...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781432</th>\n",
       "      <td>http://open-site.org/health/conditions_and_dis...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781462</th>\n",
       "      <td>ftp://ftp.apple.com/developer/tool_chest/local...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781464</th>\n",
       "      <td>http://www.reviewjournal.com/lvrj_home/2003/ju...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19954 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      url  category\n",
       "27      http://www.preplogic.com/products/exams/practi...         3\n",
       "49      http://rogerebert.suntimes.com/apps/pbcs.dll/a...         1\n",
       "53      http://www.women.com/entertain/celeb/articles/...         1\n",
       "74      http://www.admin.cam.ac.uk/univ/gsprospectus/c...        10\n",
       "86      http://www.uwbadgers.com/sport_news/mxc/headli...        14\n",
       "...                                                   ...       ...\n",
       "781380  http://web.clas.ufl.edu/users/rhatch/pages/02-...        13\n",
       "781382  http://www.free-porn-pics-free-porn-pics-free-...         0\n",
       "781432  http://open-site.org/health/conditions_and_dis...         5\n",
       "781462  ftp://ftp.apple.com/developer/tool_chest/local...         3\n",
       "781464  http://www.reviewjournal.com/lvrj_home/2003/ju...        13\n",
       "\n",
       "[19954 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = df[df['url'].apply(lambda x: len(x) > 70)]\n",
    "filtered_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85721c7-357b-455d-8577-f8a55fdc6a47",
   "metadata": {},
   "source": [
    "# Tokenizers and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e3173b-2739-48cf-81bc-819eb9845db7",
   "metadata": {},
   "source": [
    "## Character level tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2706f935-edca-4632-9415-fbcf68300ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character tokenization\n",
    "char_tokenizer = Tokenizer(char_level=True, oov_token='<OOV>')\n",
    "char_tokenizer.fit_on_texts(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67a4c253-ec28-4baa-af68-8fd7abdd7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_sequence(\n",
    "    input_text,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=\" \",\n",
    "):\n",
    "    \n",
    "    if lower:\n",
    "        input_text = input_text.lower()\n",
    "\n",
    "    translate_dict = {c: split for c in filters}\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    input_text = input_text.translate(translate_map)\n",
    "\n",
    "    seq = input_text.split(split)\n",
    "    return [i for i in seq if i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebab3535-0a29-4902-b327-35b19734fec8",
   "metadata": {},
   "source": [
    "### Word level tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290ba02-e7e2-4bde-bf56-bb469f3510aa",
   "metadata": {},
   "source": [
    "#### Calculate number of words which appear more than once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e5e0cd6-4971-4a0d-a59a-d149c92327d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_calculation():\n",
    "    word_sequences = [text_to_word_sequence(sentence) for sentence in x_train]\n",
    "    all_words_concat = [item for sublist in word_sequences for item in sublist]\n",
    "    freq_dict_words = Counter(all_words_concat)\n",
    "    \n",
    "    low_freq_words_count = 0\n",
    "    for key in freq_dict_words:\n",
    "        if freq_dict_words[key] == 1:\n",
    "            low_freq_words_count += 1\n",
    "    \n",
    "    high_freq_word_count = int(len(freq_dict_words) * (1-(low_freq_words_count/len(freq_dict_words))))\n",
    "    print(high_freq_word_count)\n",
    "    return high_freq_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6287bae-e6a6-44e0-a536-7dc684628db2",
   "metadata": {},
   "source": [
    "#### Url into sequence of word ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bda8eced-b077-4dcb-b9f4-e9684b333159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86766\n"
     ]
    }
   ],
   "source": [
    "high_freq_word_count = frequency_calculation()\n",
    "word_tokenizer = Tokenizer(oov_token='<OOV>',num_words=high_freq_word_count)\n",
    "word_tokenizer.fit_on_texts(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8fb5ec-667d-45c1-ac9b-2b3546b711b8",
   "metadata": {},
   "source": [
    "## define DataGenerator for preprocessing(tokenizing and padding) inputs on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc050e31-9d4a-42c8-a76b-39ff17fd66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, urls, labels, batch_size, char_tokenizer, word_tokenizer):\n",
    "        self.urls = urls\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.char_tokenizer = char_tokenizer\n",
    "        self.word_tokenizer = word_tokenizer\n",
    "        self.indexes = np.arange(len(urls))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.urls) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_urls = [self.urls[i] for i in batch_indexes]\n",
    "        batch_labels = [self.labels[i] for i in batch_indexes]\n",
    "        \n",
    "        char_input_data = self._preprocess_char_input(batch_urls)\n",
    "        word_input_data = self._preprocess_word_input(batch_urls)\n",
    "        word_char_input_data = self._preprocess_word_char_input(batch_urls)\n",
    "        \n",
    "        return [char_input_data, word_input_data, word_char_input_data], np.array(batch_labels)\n",
    "    \n",
    "    def _preprocess_char_input(self, urls):\n",
    "        char_sequences = self.char_tokenizer.texts_to_sequences(urls)\n",
    "        return pad_sequences(char_sequences, maxlen=MAX_URL_LEN_CHAR, padding='post',truncating='post')\n",
    "    \n",
    "    def _preprocess_word_input(self, urls):\n",
    "        word_sequences = self.word_tokenizer.texts_to_sequences(urls)\n",
    "        return pad_sequences(word_sequences, maxlen=MAX_URL_LEN_WORD, padding='post',truncating='post')\n",
    "    \n",
    "    def _preprocess_word_char_input(self, urls):\n",
    "        sentences_splitted = [text_to_word_sequence(sentence) for sentence in urls]\n",
    "    \n",
    "        for i in range(len(sentences_splitted)):\n",
    "            while len(sentences_splitted[i])<MAX_URL_LEN_CHAR:\n",
    "                sentences_splitted[i].append('<OOV>')\n",
    "        \n",
    "        words_splitted = [[word.split() for word in sentence] for sentence in sentences_splitted]\n",
    "\n",
    "        word_char_sequences = []\n",
    "        \n",
    "        for sentence in words_splitted:\n",
    "            sentence_tokenized = []\n",
    "            for i in range(len(sentence)):\n",
    "                word = self.char_tokenizer.texts_to_sequences(sentence[i])\n",
    "                word_char_padded = pad_sequences(word,maxlen=MAX_WORD_LEN,padding='post',truncating='post')\n",
    "                sentence_tokenized.append(word_char_padded)\n",
    "            word_char_sequences.append(sentence_tokenized)\n",
    "      \n",
    "        word_char_sequences = np.array(word_char_sequences)\n",
    "        word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n",
    "        \n",
    "        return word_char_sequences\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dae70f16-47ae-42b8-8bf1-9fdd3e79e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(x_train, y_train, batch_size, char_tokenizer, word_tokenizer)\n",
    "test_generator = DataGenerator(x_test, y_test, batch_size, char_tokenizer, word_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94f53c1b-a39b-40e1-a791-1a420a18b805",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = training_generator.__getitem__(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d89724c-b35f-4b6f-8082-52cbe8468fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aca24d6-1c7d-4955-8682-17ad11609390",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "837e796c-0b9f-40c4-a47b-841d60632495",
   "metadata": {},
   "source": [
    "# Character level CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74be879-6950-4b58-8cb7-476c8a46a126",
   "metadata": {},
   "source": [
    "### Character embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61540b00-775d-454d-bc19-0c8d15870246",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_input = Input((MAX_URL_LEN_CHAR,))\n",
    "\n",
    "# Embedding layer\n",
    "char_embedding = Embedding(input_dim=len(char_tokenizer.word_index) + 1, output_dim=k, input_length=MAX_URL_LEN_CHAR)(char_input)\n",
    "char_embedding = tf.expand_dims(char_embedding, -1)  # Add channel dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc5240-1a1f-4970-bc31-daf77f09f632",
   "metadata": {},
   "source": [
    "# CNN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09304aab-616f-4e73-b7c1-94c3afa40c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution and pooling for character-level\n",
    "# h = 3\n",
    "conv_3_char = Conv2D(num_filters, (3, k), activation='relu')(char_embedding)\n",
    "conv_3_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_3_char)\n",
    "# h = 4\n",
    "conv_4_char = Conv2D(num_filters, (4, k), activation='relu')(char_embedding)\n",
    "conv_4_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_4_char)\n",
    "# h = 5\n",
    "conv_5_char = Conv2D(num_filters, (5, k), activation='relu')(char_embedding)\n",
    "conv_5_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_5_char)\n",
    "# h = 6\n",
    "conv_6_char = Conv2D(num_filters, (6, k), activation='relu')(char_embedding)\n",
    "conv_6_char = MaxPooling2D((2, 1), strides=(2, 1))(conv_6_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327cf171-b6ac-469d-96c0-1a0cfc727537",
   "metadata": {},
   "source": [
    "### Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83263576-ddc6-4ee2-9d81-6274fc249abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all convolutional layer outputs\n",
    "concatenated = Concatenate(axis=1)([conv_3_char,conv_4_char,conv_5_char,conv_6_char])\n",
    "flattened = Flatten()(concatenated)\n",
    "\n",
    "# feed concatenated conv layers to fully conected layer\n",
    "dense_char = Dense(512,activation='relu')(flattened)\n",
    "dropout = Dropout(0.5)(dense_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de878ba-24e2-4a1f-971f-707e1217e7ee",
   "metadata": {},
   "source": [
    "# Word-level Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed5e13-8e51-49e9-9fc0-8a6bd18f1733",
   "metadata": {},
   "source": [
    "##### word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee2a87e9-0061-4b28-89d8-f587b1072ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 70, 16])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_input = Input(shape=(MAX_URL_LEN_WORD,))\n",
    "\n",
    "# Embedding layer\n",
    "word_embedding = Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=k, input_length=MAX_URL_LEN_WORD)(word_input)\n",
    "\n",
    "word_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c36b41-0923-4fd9-a689-5647ac5d3035",
   "metadata": {},
   "source": [
    "##### char embedding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f40d8bc1-9d92-4348-ad7f-951306206316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 70, 15, 16])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_char_input = Input(shape=(MAX_URL_LEN_CHAR,MAX_WORD_LEN))\n",
    "\n",
    "# Embedding layer\n",
    "word_char_embedding = Embedding(input_dim=len(char_tokenizer.word_index) + 1, output_dim=k, input_length=MAX_URL_LEN_CHAR)(word_char_input)\n",
    "word_char_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c0756-aee1-4a54-a1f6-9f1c653fa517",
   "metadata": {},
   "source": [
    "##### Sum over characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "39bcff9a-b91e-4115-aa68-55567725db04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 70, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled_layer = tf.keras.layers.Lambda(lambda x: tf.math.reduce_sum(x, axis=2))(word_char_embedding)\n",
    "pooled_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e6593-44e7-457a-8518-d4053b1e95d7",
   "metadata": {},
   "source": [
    "#### Element-wise Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70647042-c576-4d73-a11a-ef44ed6f5396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 70, 16, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addition_layer = Add()([pooled_layer, word_embedding])\n",
    "addition_layer = tf.expand_dims(addition_layer,-1)\n",
    "\n",
    "addition_layer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315cd81-7ce6-4ba7-a08c-720deaf0e572",
   "metadata": {},
   "source": [
    "## Word-level CNN Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f273801-80ae-4f8b-a88b-f852554a5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution and pooling for word-level\n",
    "# h = 3\n",
    "conv_3_word = Conv2D(num_filters, (3, k), activation='relu')(addition_layer)\n",
    "conv_3_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_3_word)\n",
    "# h = 4\n",
    "conv_4_word = Conv2D(num_filters, (4, k), activation='relu')(addition_layer)\n",
    "conv_4_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_4_word)\n",
    "# h = 5\n",
    "conv_5_word = Conv2D(num_filters, (5, k), activation='relu')(addition_layer)\n",
    "conv_5_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_5_word)\n",
    "# h = 6\n",
    "conv_6_word = Conv2D(num_filters, (6, k), activation='relu')(addition_layer)\n",
    "conv_6_word = MaxPooling2D((2, 1), strides=(2, 1))(conv_6_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a7e21-a6da-4370-90fe-ea4630a8b8a1",
   "metadata": {},
   "source": [
    "## Word level fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46f6d25b-89f8-480c-b4fd-b7a5caadf213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 512])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate all convolutional layer outputs\n",
    "concatenated_word = Concatenate(axis=1)([conv_3_word,conv_4_word,conv_5_word,conv_6_word])\n",
    "flattened_word = Flatten()(concatenated_word)\n",
    "\n",
    "# feed concatenated conv layers to fully conected layer\n",
    "dense_word = Dense(512,activation='relu')(flattened_word)\n",
    "dropout_word = Dropout(0.5)(dense_word)\n",
    "dropout_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40542eaf-16c8-422c-a74f-2404b82f803a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb4e3625-ac35-42be-9bf5-c1b357c64130",
   "metadata": {},
   "source": [
    "## concatenate outputs of char-level and word-level blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "909e1e11-355e-4581-ace8-e352b24d5304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 1024])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenate_all = Concatenate()([dropout,dropout_word])\n",
    "concatenate_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0051b07-0774-4c98-8476-e3639aff5abf",
   "metadata": {},
   "source": [
    "# last fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f3a8963-9f6d-4351-8881-52af23d07ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 128])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_1 = Dense(512,activation='relu')(concatenate_all)\n",
    "dropout_fc_1 = Dropout(0.5)(fc_1)\n",
    "\n",
    "fc_2 = Dense(256,activation='relu')(dropout_fc_1)\n",
    "dropout_fc_2 = Dropout(0.5)(fc_2)\n",
    "\n",
    "fc_3 = Dense(128,activation='relu')(dropout_fc_2)\n",
    "dropout_fc_3 = Dropout(0.5)(fc_3)\n",
    "dropout_fc_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2386c9-6f16-4b23-998a-abcaeecc068e",
   "metadata": {},
   "source": [
    "#### output layer with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "179c97ff-8d75-4bdc-8008-ee1a4244cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Dense(num_classes, activation='softmax')(dropout_fc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea5e7c-57b4-499b-80e2-5f95df2f3186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c74a465-0c35-4c17-9b50-0044de8810dc",
   "metadata": {},
   "source": [
    "# Model compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "365d22b8-0c38-4644-89f9-55e6e6fb38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce493f05-7754-4bbb-aa90-0b6a4ecb94f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 70, 15)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 70, 15, 16)   1104        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 70)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None, 70, 16)       0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 70, 16)       9026224     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 70, 16)       1104        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 70, 16)       0           lambda[0][0]                     \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims (TFOpLambda)     (None, 70, 16, 1)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf.expand_dims_1 (TFOpLambda)   (None, 70, 16, 1)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 68, 1, 256)   12544       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 67, 1, 256)   16640       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 66, 1, 256)   20736       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 65, 1, 256)   24832       tf.expand_dims[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 68, 1, 256)   12544       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 67, 1, 256)   16640       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 66, 1, 256)   20736       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 65, 1, 256)   24832       tf.expand_dims_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 34, 1, 256)   0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 33, 1, 256)   0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 33, 1, 256)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 32, 1, 256)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 34, 1, 256)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 33, 1, 256)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 33, 1, 256)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 1, 256)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 132, 1, 256)  0           max_pooling2d[0][0]              \n",
      "                                                                 max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 132, 1, 256)  0           max_pooling2d_4[0][0]            \n",
      "                                                                 max_pooling2d_5[0][0]            \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 33792)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 33792)        0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          17302016    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          17302016    flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1024)         0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          524800      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          131328      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          32896       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 128)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 15)           1935        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 44,472,927\n",
      "Trainable params: 44,472,927\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[char_input, word_input, word_char_input],outputs=output)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df285a84-9fb4-493d-bb6e-c27021cf5c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = x_train[:2000]\n",
    "# y_train = y_train[:2000]\n",
    "# len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bf41319-8091-4397-87ca-6565515b3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fee01a17-7025-488a-9c26-35a0e85432d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# def preprocess_char_input(urls):\n",
    "#     char_sequences =char_tokenizer.texts_to_sequences(urls)\n",
    "#     return pad_sequences(char_sequences, maxlen=MAX_URL_LEN_CHAR, padding='post')\n",
    "\n",
    "# def preprocess_word_input( urls):\n",
    "#     word_sequences = word_tokenizer.texts_to_sequences(urls)\n",
    "#     return pad_sequences(word_sequences, maxlen=MAX_URL_LEN_WORD, padding='post')\n",
    "\n",
    "# def preprocess_word_char_input( urls):\n",
    "#     sentences_splitted = [text_to_word_sequence(sentence) for sentence in urls]\n",
    "\n",
    "#     for i in range(len(sentences_splitted)):\n",
    "#         while len(sentences_splitted[i])<100:\n",
    "#             sentences_splitted[i].append('<PAD>')\n",
    "    \n",
    "#     words_splitted = [[word.split() for word in sentence] for sentence in sentences_splitted]\n",
    "\n",
    "#     word_char_sequences = []\n",
    "    \n",
    "#     for sentence in words_splitted:\n",
    "#         sentence_tokenized = []\n",
    "#         for i in range(len(sentence)):\n",
    "#             word = char_tokenizer.texts_to_sequences(sentence[i])\n",
    "#             word_char_padded = pad_sequences(word,maxlen=MAX_WORD_LEN,padding='post')\n",
    "#             sentence_tokenized.append(word_char_padded)\n",
    "#         word_char_sequences.append(sentence_tokenized)\n",
    "  \n",
    "#     word_char_sequences = np.array(word_char_sequences)\n",
    "#     word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n",
    "    \n",
    "#     return word_char_sequences\n",
    "# char_input_data = preprocess_char_input(urls)\n",
    "# word_input_data = preprocess_word_input(urls)\n",
    "# word_char_input_data = preprocess_word_char_input(urls)\n",
    "        \n",
    "# print(char_input_data)\n",
    "# print(word_input_data)\n",
    "# print(word_char_input_data)\n",
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce833b10-8bff-40e0-8ccd-b146fe9f94ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " 4041/19538 [=====>........................] - ETA: 7:14 - loss: 2.2045 - accuracy: 0.3030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\warfa\\AppData\\Local\\Temp\\ipykernel_24416\\3722087896.py:51: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  word_char_sequences = np.array(word_char_sequences)\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "2 root error(s) found.\n  (0) Unknown:  IndexError: tuple index out of range\nTraceback (most recent call last):\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 892, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 822, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 948, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\warfa\\AppData\\Local\\Temp\\ipykernel_24416\\3722087896.py\", line 20, in __getitem__\n    word_char_input_data = self._preprocess_word_char_input(batch_urls)\n\n  File \"C:\\Users\\warfa\\AppData\\Local\\Temp\\ipykernel_24416\\3722087896.py\", line 52, in _preprocess_word_char_input\n    word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n\nIndexError: tuple index out of range\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n  (1) Unknown:  IndexError: tuple index out of range\nTraceback (most recent call last):\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 892, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 822, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 948, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\warfa\\AppData\\Local\\Temp\\ipykernel_24416\\3722087896.py\", line 20, in __getitem__\n    word_char_input_data = self._preprocess_word_char_input(batch_urls)\n\n  File \"C:\\Users\\warfa\\AppData\\Local\\Temp\\ipykernel_24416\\3722087896.py\", line 52, in _preprocess_word_char_input\n    word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n\nIndexError: tuple index out of range\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_6]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2276]\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# [char_input_data, word_input_data, word_char_input_data],\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# y_train,\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\keras\\engine\\training.py:1184\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1179\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1180\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1181\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1182\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1183\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1184\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1185\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1186\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    915\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 917\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    920\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    921\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3037\u001b[0m   (graph_function,\n\u001b[0;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m     args,\n\u001b[0;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1968\u001b[0m     executing_eagerly)\n\u001b[0;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown:  IndexError: tuple index out of range\nTraceback (most recent call last):\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 892, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 822, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 948, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\warfa\\AppData\\Local\\Temp\\ipykernel_24416\\3722087896.py\", line 20, in __getitem__\n    word_char_input_data = self._preprocess_word_char_input(batch_urls)\n\n  File \"C:\\Users\\warfa\\AppData\\Local\\Temp\\ipykernel_24416\\3722087896.py\", line 52, in _preprocess_word_char_input\n    word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n\nIndexError: tuple index out of range\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n  (1) Unknown:  IndexError: tuple index out of range\nTraceback (most recent call last):\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 249, in __call__\n    ret = func(*args)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 645, in wrapper\n    return func(*args, **kwargs)\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\", line 892, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 822, in wrapped_generator\n    for data in generator_fn():\n\n  File \"C:\\Users\\warfa\\.conda\\envs\\AI\\lib\\site-packages\\keras\\engine\\data_adapter.py\", line 948, in generator_fn\n    yield x[i]\n\n  File \"C:\\Users\\warfa\\AppData\\Local\\Temp\\ipykernel_24416\\3722087896.py\", line 20, in __getitem__\n    word_char_input_data = self._preprocess_word_char_input(batch_urls)\n\n  File \"C:\\Users\\warfa\\AppData\\Local\\Temp\\ipykernel_24416\\3722087896.py\", line 52, in _preprocess_word_char_input\n    word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n\nIndexError: tuple index out of range\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_6]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_2276]\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    training_generator,\n",
    "    # [char_input_data, word_input_data, word_char_input_data],\n",
    "    # y_train,\n",
    "    validation_data=test_generator,\n",
    "    epochs=2,\n",
    "    callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1805c7a3-c000-47df-b64e-ed24dc0dd92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_freq=VAL_FREQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "274583bf-f94d-4fb3-a4db-8105a71f5d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[ 8,  3,  3,  9, 17,  2,  2, 20, 14,  5, 13, 12, 19, 22,  5, 10,\n",
       "           7, 12,  2,  3, 13,  3, 18,  6,  2,  3,  3, 28, 28, 29, 41, 38,\n",
       "          42, 41,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0]]),\n",
       "  array([[ 2, 24, 32,  4, 49,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "           0,  0,  0,  0,  0,  0]]),\n",
       "  array([[[ 8,  3,  3, ...,  0,  0,  0],\n",
       "          [20, 14,  0, ...,  0,  0,  0],\n",
       "          [13, 12, 19, ...,  0,  0,  0],\n",
       "          ...,\n",
       "          [ 1,  9, 11, ...,  0,  0,  0],\n",
       "          [ 1,  9, 11, ...,  0,  0,  0],\n",
       "          [ 1,  9, 11, ...,  0,  0,  0]]])],\n",
       " array([1], dtype=int64))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(classes)\n",
    "training_generator.__getitem__(175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f021fff6-3522-4f21-86a5-d741f5b7a2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 8,  3,  3,  9, 17,  2,  2,  4,  4,  4,  5, 23, 18, 11,  3,  4,\n",
      "         7,  7, 19, 14, 10,  8, 20, 15, 10,  8,  7, 23, 10,  8, 15, 13,\n",
      "        14,  3,  5,  7, 15, 21,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0]]), array([[2, 3, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0]]), array([[[ 8,  3,  3, ...,  0,  0,  0],\n",
      "        [ 4,  4,  4, ...,  0,  0,  0],\n",
      "        [14, 10,  8, ..., 13, 14,  3],\n",
      "        ...,\n",
      "        [ 1,  9, 11, ...,  0,  0,  0],\n",
      "        [ 1,  9, 11, ...,  0,  0,  0],\n",
      "        [ 1,  9, 11, ...,  0,  0,  0]]])]\n"
     ]
    }
   ],
   "source": [
    "item = training_generator.__getitem__(17571)[0]\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2a780a83-00c5-4199-a9a7-44def344819e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 70, 15)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# char_tokenizer.sequences_to_texts(item[2])\n",
    "# word_tokenizer.sequences_to_texts(item[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53c12879-2020-4e5f-a6ae-34aba7171fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[21, 11, 12, 11, 15, 33, 7, 22, 11]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tokenizer.texts_to_sequences(['gamarjoba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec7cd475-9431-4d26-b14a-7454284bbbab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['g a m a r j o b a']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_tokenizer.sequences_to_texts([[21, 11, 12, 11, 15, 33, 7, 22, 11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7c16f-8072-477f-9dfa-498b32aa9c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word_char_input(urls):\n",
    "        sentences_splitted = [text_to_word_sequence(sentence) for sentence in urls]\n",
    "    \n",
    "        for i in range(len(sentences_splitted)):\n",
    "            while len(sentences_splitted[i])<MAX_URL_LEN_CHAR:\n",
    "                sentences_splitted[i].append('<OOV>')\n",
    "        \n",
    "        words_splitted = [[word.split() for word in sentence] for sentence in sentences_splitted]\n",
    "\n",
    "        word_char_sequences = []\n",
    "        \n",
    "        for sentence in words_splitted:\n",
    "            sentence_tokenized = []\n",
    "            for i in range(len(sentence)):\n",
    "                word = self.char_tokenizer.texts_to_sequences(sentence[i])\n",
    "                word_char_padded = pad_sequences(word,maxlen=MAX_WORD_LEN,padding='post',truncating='post')\n",
    "                sentence_tokenized.append(word_char_padded)\n",
    "            word_char_sequences.append(sentence_tokenized)\n",
    "      \n",
    "        word_char_sequences = np.array(word_char_sequences)\n",
    "        word_char_sequences = word_char_sequences.reshape(word_char_sequences.shape[0],word_char_sequences.shape[1],word_char_sequences.shape[-1])\n",
    "        \n",
    "        return word_char_sequences\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
